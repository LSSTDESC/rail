{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Utilities\n",
    "\n",
    "**Authors:** Olivia Lynn\n",
    "\n",
    "**Last Run Successfully:** September 20, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that contains various utilities that may be used when working with RAIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing imported stages (1/2)\n",
    "\n",
    "Let's list out our currently imported stages. Right now, this will only be what we get by importing `rail` and `rail.stages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rail.stages\n",
    "for val in rail.core.RailStage.pipeline_stages.values():\n",
    "    print(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and attach all\n",
    "\n",
    "Using `rail.stages.import_and_attach_all()` lets you import all packages within the RAIL ecosystem at once. \n",
    "\n",
    "This kind of blanket import is a useful shortcut; however, it will be slower than specific imports, as you will import things you'll never need. \n",
    "\n",
    "As of such, `import_and_attach_all` is recommended for new users and those who wish to do rapid exploration with notebooks; pipelines designed to be run at scale would generally prefer lightweight, specific imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rail\n",
    "import rail.stages\n",
    "rail.stages.import_and_attach_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've attached all available stages to rail.stages, we can use `from rail.stages import *` to let us omit prefixes. \n",
    "\n",
    "To see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with prefix\n",
    "\n",
    "print(rail.core.util_stages.ColumnMapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without prefix\n",
    "\n",
    "try:\n",
    "    print(ColumnMapper)\n",
    "except Exception as e: \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.stages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ColumnMapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing imported stages (2/2)\n",
    "\n",
    "Now, let's try listing imported stages again.\n",
    "\n",
    "Note that we can now just call `RailStage` instead of `rail.core.RailStage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in RailStage.pipeline_stages.values():\n",
    "    print(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this list of imported stages to browse for specifics, such as looking through our available estimators.\n",
    "\n",
    "**Note:** this will only filter through what you've imported, so if you haven't imported everything above, this will not be a complete list of all estimators available in RAIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in RailStage.pipeline_stages.values():\n",
    "    if issubclass(val[0], rail.estimation.estimator.CatEstimator):\n",
    "        print(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing keys in the Data Store (1/2)\n",
    "\n",
    "Let's list out the keys in the Data Store to see what data we have stored.\n",
    "\n",
    "First, we must set up the Data Store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because we've only just created the store, as you may have guessed, it is empty. \n",
    "\n",
    "We'll come back to this in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding data files with find_rail_file\n",
    "\n",
    "We need to define our flow file that we'll use in our pipeline\n",
    "\n",
    "If we already know its path, we can just point directly to the file (relative to the directory that holds our `rail/` directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core.utils import RAILDIR\n",
    "\n",
    "flow_file = os.path.join(\n",
    "    RAILDIR, \"rail/examples_data/goldenspike_data/data/pretrained_flow.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we aren't sure where our file is (or we're just feeling lazy) we can use `find_rail_file`.\n",
    "\n",
    "This is especially helpful in cases where our installation is spread out, and some rail modules are located separately from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core.utils import find_rail_file\n",
    "\n",
    "flow_file = find_rail_file('examples_data/goldenspike_data/data/pretrained_flow.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set our FLOWDIR based on the location of our flow file, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['FLOWDIR'] = os.path.dirname(flow_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have to set up some other variables for our pipeline:\n",
    "\n",
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "band_dict = {band: f\"mag_{band}_lsst\" for band in bands}\n",
    "rename_dict = {f\"mag_{band}_lsst_err\": f\"mag_err_{band}_lsst\" for band in bands}\n",
    "post_grid = [float(x) for x in np.linspace(0.0, 5, 21)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ceci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some stages\n",
    "\n",
    "flow_engine_test = FlowCreator.make_stage(\n",
    "    name=\"flow_engine_test\", model=flow_file, n_samples=50\n",
    ")\n",
    "col_remapper_test = ColumnMapper.make_stage(\n",
    "    name=\"col_remapper_test\", hdf5_groupname=\"\", columns=rename_dict\n",
    ")\n",
    "#flow_engine_test.sample(6, seed=0).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the stages to the pipeline\n",
    "\n",
    "pipe = ceci.Pipeline.interactive()\n",
    "stages = [flow_engine_test, col_remapper_test]\n",
    "for stage in stages:\n",
    "    pipe.add_stage(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect stages\n",
    "\n",
    "col_remapper_test.connect_input(flow_engine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspecting the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing keys in the Data Store (2/2)\n",
    "\n",
    "Now that we have a some data in the Data Store, let's take another look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting names of stages in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.stage_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the configuration of a particular stage\n",
    "\n",
    "Let's take a look a the config of the first stage we just listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.flow_engine_test.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a configuration value\n",
    " \n",
    "We can update config values even after the stage has been created. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.flow_engine_test.config.update(seed=42)\n",
    "\n",
    "pipe.flow_engine_test.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing stage outputs (as both tags and aliased tags)\n",
    "\n",
    "Let's get the list of outputs as 'tags'.\n",
    "\n",
    "These are how the stage thinks of the outputs, as a list names associated to DataHandle types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.flow_engine_test.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the list of outputs as 'aliased tags'.\n",
    "\n",
    "These are how the pipeline thinks of the outputs, as a unique key that points to a particular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.flow_engine_test._outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all pipeline methods and parameters that can be set\n",
    "\n",
    "If you'd like to take a closer look at what you can do with a pipeline, use `dir(pipe)` to list out available methods and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dir(pipe):\n",
    "    if '__' not in item:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toggling resume mode\n",
    "\n",
    "We can turn 'resume mode' on when initializing a pipeline.\n",
    "\n",
    "Resume mode lets us skip stages that already have output files, so we don't have to rerun the same stages as we iterate on a pipeline.\n",
    "\n",
    "Just add a `resume=True` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.initialize(\n",
    "    dict(model=flow_file), dict(output_dir=\".\", log_dir=\".\", resume=True), None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `pipe.stages` should show order of classes, or all the stages this pipeline will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing notebooks with git\n",
    "\n",
    "_(thank you to https://stackoverflow.com/a/58004619)_\n",
    "\n",
    "You can modify your git settings to run a filter over certain files before they are added to git. This will leave the original file on disk as-is, but commit the \"cleaned\" version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, add the following to your local `.git/config` file (or global `~/.gitconfig`):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[filter \"strip-notebook-output\"]\n",
    "    clean = \"jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to=notebook --stdin --stdout --log-level=ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a `.gitattributes` file in your directory with notebooks and add the following line:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*.ipynb filter=strip-notebook-output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
