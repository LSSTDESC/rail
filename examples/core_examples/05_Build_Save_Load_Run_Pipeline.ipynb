{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build, Save, Load, and Run a Pipeline\n",
    "\n",
    "**Author:** Eric Charles\n",
    "\n",
    "**Last Run Successfully:** December 22, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to:\n",
    "\n",
    "1. Build a simple interactive rail pipeline,\n",
    "\n",
    "2. Save that pipeline (including configuration) to a yaml file,\n",
    "\n",
    "3. Load that pipeline from the saved yaml file,\n",
    "\n",
    "4. Run the loaded pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import ceci\n",
    "import rail\n",
    "import tables_io\n",
    "from rail.core.stage import RailStage\n",
    "from rail.creation.degraders.spectroscopic_degraders import LineConfusion\n",
    "from rail.creation.degraders.quantityCut import QuantityCut\n",
    "from rail.creation.degraders.photometric_errors import LSSTErrorModel\n",
    "from rail.creation.engines.flowEngine import FlowCreator, FlowPosterior\n",
    "from rail.tools.flow_handle import FlowHandle\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.tools.table_tools import ColumnMapper, TableConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the pipeline\n",
    "\n",
    "### Some configuration setup\n",
    "\n",
    "The example pipeline builds some of the RAIL creation functionality into a pipeline. \n",
    "\n",
    "Here we are defining:\n",
    "\n",
    "1. The location of the pretrained PZFlow file used with this example.\n",
    "\n",
    "2. The bands we will be generating data for.\n",
    "\n",
    "3. The names of the columns where we will be writing the error estimates.\n",
    "\n",
    "4. The grid of redshifts we use for posterior estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.utils.path_utils import find_rail_file\n",
    "\n",
    "flow_file = find_rail_file(\"examples_data/goldenspike_data/data/pretrained_flow.pkl\")\n",
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "band_dict = {band: f\"mag_{band}_lsst\" for band in bands}\n",
    "rename_dict = {f\"mag_{band}_lsst_err\": f\"mag_err_{band}_lsst\" for band in bands}\n",
    "post_grid = [float(x) for x in np.linspace(0.0, 5, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline stages\n",
    "\n",
    "The RailStage base class defines the `make_stage` \"classmethod\" function, which allows us to make a stage of\n",
    "that particular type in a general way.\n",
    "\n",
    "Note that that we are passing in the configuration parameters to each pipeline stage as keyword arguments.\n",
    "\n",
    "The names of the parameters will depend on the stage type.\n",
    "\n",
    "A couple of things are important:\n",
    "\n",
    "1. Each stage should have a unique name. In `ceci`, stage names default to the name of the class (e.g., FlowCreator, or LSSTErrorModel); this would be problematic if you wanted two stages of the same type in a given pipeline, so be sure to assign each stage its own name.\n",
    "\n",
    "2. At this point, we aren't actually worrying about the connections between the stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data handle of the file \n",
    "flow_handle = FlowHandle(\"model\", path=flow_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  model: <class 'rail.tools.flow_handle.FlowHandle'> /home/jscora/code/desc-rail/rail_base/src/rail/examples_data/goldenspike_data/data/pretrained_flow.pkl, (w), flow_engine_test\n"
     ]
    }
   ],
   "source": [
    "flow_engine_test = FlowCreator.make_stage(\n",
    "    name=\"flow_engine_test\", model=flow_handle, n_samples=50\n",
    ")\n",
    "\n",
    "lsst_error_model_test = LSSTErrorModel.make_stage(\n",
    "    name=\"lsst_error_model_test\", renameDict=band_dict\n",
    ")\n",
    "\n",
    "col_remapper_test = ColumnMapper.make_stage(\n",
    "    name=\"col_remapper_test\", hdf5_groupname=\"\", columns=rename_dict\n",
    ")\n",
    "\n",
    "flow_post_test = FlowPosterior.make_stage(\n",
    "    name=\"flow_post_test\", column=\"redshift\", flow=flow_file, grid=post_grid\n",
    ")\n",
    "\n",
    "table_conv_test = TableConverter.make_stage(\n",
    "    name=\"table_conv_test\", output_format=\"numpyDict\", seed=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  output_flow_engine_test: inprogress_output_flow_engine_test.pq, flow_engine_test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag_z_lsst</th>\n",
       "      <th>mag_y_lsst</th>\n",
       "      <th>mag_r_lsst</th>\n",
       "      <th>mag_i_lsst</th>\n",
       "      <th>redshift</th>\n",
       "      <th>mag_u_lsst</th>\n",
       "      <th>mag_g_lsst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.734173</td>\n",
       "      <td>22.580509</td>\n",
       "      <td>24.027840</td>\n",
       "      <td>22.996883</td>\n",
       "      <td>0.739296</td>\n",
       "      <td>25.822563</td>\n",
       "      <td>25.088484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.666660</td>\n",
       "      <td>22.415169</td>\n",
       "      <td>23.995960</td>\n",
       "      <td>23.025354</td>\n",
       "      <td>0.644894</td>\n",
       "      <td>27.391764</td>\n",
       "      <td>25.447910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.013422</td>\n",
       "      <td>20.779903</td>\n",
       "      <td>21.757725</td>\n",
       "      <td>21.298742</td>\n",
       "      <td>0.348834</td>\n",
       "      <td>23.999668</td>\n",
       "      <td>22.884043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.050123</td>\n",
       "      <td>23.818220</td>\n",
       "      <td>24.271830</td>\n",
       "      <td>24.152014</td>\n",
       "      <td>1.623727</td>\n",
       "      <td>24.338676</td>\n",
       "      <td>24.272142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.140982</td>\n",
       "      <td>23.095510</td>\n",
       "      <td>23.577923</td>\n",
       "      <td>23.190630</td>\n",
       "      <td>0.551647</td>\n",
       "      <td>25.136570</td>\n",
       "      <td>24.586559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.022926</td>\n",
       "      <td>20.927565</td>\n",
       "      <td>22.416426</td>\n",
       "      <td>21.435768</td>\n",
       "      <td>0.839687</td>\n",
       "      <td>23.225256</td>\n",
       "      <td>23.068544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mag_z_lsst  mag_y_lsst  mag_r_lsst  mag_i_lsst  redshift  mag_u_lsst  \\\n",
       "0   22.734173   22.580509   24.027840   22.996883  0.739296   25.822563   \n",
       "1   22.666660   22.415169   23.995960   23.025354  0.644894   27.391764   \n",
       "2   21.013422   20.779903   21.757725   21.298742  0.348834   23.999668   \n",
       "3   24.050123   23.818220   24.271830   24.152014  1.623727   24.338676   \n",
       "4   23.140982   23.095510   23.577923   23.190630  0.551647   25.136570   \n",
       "5   21.022926   20.927565   22.416426   21.435768  0.839687   23.225256   \n",
       "\n",
       "   mag_g_lsst  \n",
       "0   25.088484  \n",
       "1   25.447910  \n",
       "2   22.884043  \n",
       "3   24.272142  \n",
       "4   24.586559  \n",
       "5   23.068544  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_engine_test.sample(6, seed=0).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the pipeline and add the stages\n",
    "\n",
    "Here we make an empty interactive pipeline (interactive in the sense that it will be run locally, rather than using the batch submission mechanisms built into `ceci`), and add the stages to that pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ceci.Pipeline.interactive()\n",
    "stages = [flow_engine_test, lsst_error_model_test, col_remapper_test, table_conv_test]\n",
    "for stage in stages:\n",
    "    pipe.add_stage(stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive introspection\n",
    "\n",
    "Here are some examples of interactive introspection into the pipeline\n",
    "\n",
    "I.e., some functions that you can use to figure out what the pipeline is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flow_engine_test',\n",
       " 'lsst_error_model_test',\n",
       " 'col_remapper_test',\n",
       " 'table_conv_test']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the names of the stages\n",
    "pipe.stage_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StageConfig{output_mode:default,n_samples:6,seed:0,name:flow_engine_test,model:/home/jscora/code/desc-rail/rail_base/src/rail/examples_data/goldenspike_data/data/pretrained_flow.pkl,config:None,}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the configuration of a particular stage\n",
    "pipe.flow_engine_test.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('output', rail.core.data.PqHandle)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of outputs 'tags'\n",
    "# These are how the stage thinks of the outputs, as a list names associated to DataHandle types.\n",
    "pipe.flow_engine_test.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_flow_engine_test': 'output_flow_engine_test.pq'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of outputs 'aliased tags'\n",
    "# These are how the pipeline things of the outputs, as a unique key that points to a particular file\n",
    "pipe.flow_engine_test._outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect up the pipeline stages\n",
    "\n",
    "We can use the `RailStage.connect_input` function to connect one stage to another.\n",
    "By default, this will connect the output data product called `output` for one stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  output_flow_engine_test: None, lsst_error_model_test\n",
      "Inserting handle into data store.  output_lsst_error_model_test: inprogress_output_lsst_error_model_test.pq, lsst_error_model_test\n",
      "Inserting handle into data store.  output_lsst_error_model_test: None, col_remapper_test\n",
      "Inserting handle into data store.  output_col_remapper_test: inprogress_output_col_remapper_test.pq, col_remapper_test\n",
      "Inserting handle into data store.  output_col_remapper_test: None, table_conv_test\n"
     ]
    }
   ],
   "source": [
    "lsst_error_model_test.connect_input(flow_engine_test)\n",
    "col_remapper_test.connect_input(lsst_error_model_test)\n",
    "# flow_post_test.connect_input(col_remapper_test, inputTag='input')\n",
    "table_conv_test.connect_input(col_remapper_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the pipeline\n",
    "\n",
    "This will do a few things:\n",
    "\n",
    "1. Attach any global pipeline inputs that were not specified in the connections above. In our case, the input flow file is pre-existing and must be specified as a global input.\n",
    "\n",
    "2. Specifiy output and logging directories.\n",
    "\n",
    "3. Optionally, create the pipeline in 'resume' mode, where it will ignore stages if all of their output already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'flow_engine_test': <Job flow_engine_test>,\n",
       "   'lsst_error_model_test': <Job lsst_error_model_test>,\n",
       "   'col_remapper_test': <Job col_remapper_test>,\n",
       "   'table_conv_test': <Job table_conv_test>},\n",
       "  [<rail.creation.engines.flowEngine.FlowCreator at 0x707a84a92780>,\n",
       "   <rail.creation.degraders.photometric_errors.LSSTErrorModel at 0x707a8011f650>,\n",
       "   Stage that applies remaps the following column names in a pandas DataFrame:\n",
       "   f{str(self.config.columns)},\n",
       "   <rail.tools.table_tools.TableConverter at 0x707a80294c80>]),\n",
       " {'output_dir': '.', 'log_dir': '.', 'resume': False})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.initialize(\n",
    "    dict(model=flow_file), dict(output_dir=\".\", log_dir=\".\", resume=False), None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the pipeline\n",
    "\n",
    "This will actually write two files (as this is what `ceci` wants)\n",
    "\n",
    "1. `pipe_example.yml`, which will have a list of stages, with instructions on how to execute the stages (e.g., run this stage in parallel on 20 processors). For an interactive pipeline, those instructions will be trivial.\n",
    "\n",
    "2. `pipe_example_config.yml`, which will have a dictionary of configurations for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save(\"pipe_saved.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the saved pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  model: /home/jscora/code/desc-rail/rail_base/src/rail/examples_data/goldenspike_data/data/pretrained_flow.pkl, flow_engine_test\n"
     ]
    }
   ],
   "source": [
    "pr = ceci.Pipeline.read(\"pipe_saved.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the newly read pipeline\n",
    "\n",
    "This will actually launch a Unix process to individually run each stage of the pipeline; you can see the commands that are being executed in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing flow_engine_test\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.creation.engines.flowEngine.FlowCreator   --model=/home/jscora/code/desc-rail/rail_base/src/rail/examples_data/goldenspike_data/data/pretrained_flow.pkl   --name=flow_engine_test   --config=pipe_saved_config.yml   --output=./output_flow_engine_test.pq \n",
      "Output writing to ./flow_engine_test.out\n",
      "\n",
      "Job flow_engine_test has completed successfully in 6.0 seconds seconds !\n",
      "\n",
      "Executing lsst_error_model_test\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.creation.degraders.photometric_errors.LSSTErrorModel   --input=./output_flow_engine_test.pq   --name=lsst_error_model_test   --config=pipe_saved_config.yml   --output=./output_lsst_error_model_test.pq \n",
      "Output writing to ./lsst_error_model_test.out\n",
      "\n",
      "Job lsst_error_model_test has completed successfully in 3.0 seconds seconds !\n",
      "\n",
      "Executing col_remapper_test\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.tools.table_tools.ColumnMapper   --input=./output_lsst_error_model_test.pq   --name=col_remapper_test   --config=pipe_saved_config.yml   --output=./output_col_remapper_test.pq \n",
      "Output writing to ./col_remapper_test.out\n",
      "\n",
      "Job col_remapper_test has completed successfully in 3.0 seconds seconds !\n",
      "\n",
      "Executing table_conv_test\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.tools.table_tools.TableConverter   --input=./output_col_remapper_test.pq   --name=table_conv_test   --config=pipe_saved_config.yml   --output=./output_table_conv_test.hdf5 \n",
      "Output writing to ./table_conv_test.out\n",
      "\n",
      "Job table_conv_test has completed successfully in 3.0 seconds seconds !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running saved pipelines from the command line\n",
    "Once you've saved a pipeline and have the `pipeline_name.yml` and `pipeline_name_config.yml` file pair, you can go ahead and run the pipeline from the command line instead, if you prefer. With [ceci](https://github.com/LSSTDESC/ceci) installed in your environment, just run `ceci path/to/the/pipeline.yml`. Running the pipeline we've just made would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ceci pipe_saved.yml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
