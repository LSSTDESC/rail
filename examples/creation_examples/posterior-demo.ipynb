{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Creator to Calculate True Posteriors for a Galaxy Sample\n",
    "\n",
    "author: John Franklin Crenshaw, Sam Schmidt, Eric Charles, others...\n",
    "\n",
    "last run successfully: August 2, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use a RAIL Engine to calculate true posteriors for galaxy samples drawn from the same Engine. Note that this notebook assumes you have already read through `degradation-demo.ipynb`.\n",
    "\n",
    "Calculating posteriors is more complicated than drawing samples, because it requires more knowledge of the engine that underlies the Engine. In this example, we will use the same engine we used in Degradation demo: `FlowEngine` which wraps a normalizing flow from the [pzflow](https://github.com/jfcrenshaw/pzflow) package.\n",
    "\n",
    "This notebook will cover three scenarios of increasing complexity:\n",
    "\n",
    "1. Calculating posteriors without errors\n",
    "\n",
    "2. Calculating posteriors while convolving errors\n",
    "\n",
    "3. Calculating posteriors with missing bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pzflow.examples import get_example_flow\n",
    "from rail.creation.engines.flowEngine import FlowCreator, FlowPosterior\n",
    "from rail.creation.degradation.lsst_error_model import LSSTErrorModel\n",
    "from rail.creation.degradation.quantityCut import QuantityCut\n",
    "from rail.creation.degradation.spectroscopic_degraders import (\n",
    "    InvRedshiftIncompleteness,\n",
    "    LineConfusion,\n",
    ")\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.util_stages import ColumnMapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pzflow\n",
    "import os\n",
    "\n",
    "flow_file = os.path.join(\n",
    "    os.path.dirname(pzflow.__file__), \"example_files\", \"example-flow.pzflow.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the Rail data store.  RAIL uses [ceci](https://github.com/LSSTDESC/ceci), which is designed for pipelines rather than interactive notebooks, the data store will work around that and enable us to use data interactively.  See the `rail/examples/goldenspike_examples/goldenspike.ipynb` example notebook for more details on the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculating posteriors without errors\n",
    "\n",
    "For a basic first example, let's make a Creator with no degradation and draw a sample.\n",
    "\n",
    "Note that the `FlowEngine.sample` method is handing back a `DataHandle`.  When talking to rail stages we can use this as though it were the underlying data and pass it as an argument.  This allows the rail stages to keep track of where their inputs are coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 6\n",
    "# create the FlowCreator\n",
    "flowCreator = FlowCreator.make_stage(name=\"truth\", model=flow_file, n_samples=n_samples)\n",
    "# draw a few samples\n",
    "samples_truth = flowCreator.sample(6, seed=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate true posteriors for this sample. Note the important fact here: these are literally the true posteriors for the sample because pzflow gives us direct access to the probability distribution from which the sample was drawn!\n",
    "\n",
    "When calculating posteriors, the Engine will always require `data`, which is a pandas DataFrame of the galaxies for which we are calculating posteriors (in out case the `samples_truth`). Because we are using a `FlowEngine`, we also must provide `grid`, because `FlowEngine` calculates posteriors over a grid of redshift values.\n",
    "\n",
    "Let's calculate posteriors for every galaxy in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_post = FlowPosterior.make_stage(\n",
    "    name=\"truth_post\",\n",
    "    column=\"redshift\",\n",
    "    grid=np.linspace(0, 2.5, 100),\n",
    "    marg_rules=dict(flag=np.nan, u=lambda row: np.linspace(25, 31, 10)),\n",
    "    flow=flow_file,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs = flow_post.get_posterior(samples_truth, column=\"redshift\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Creator returns the pdfs as a [qp](https://github.com/LSSTDESC/qp) Ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfs.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these pdfs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, constrained_layout=True, dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # plot the pdf\n",
    "    pdfs.data[i].plot_native(axes=ax)\n",
    "\n",
    "    # plot the true redshift\n",
    "    ax.axvline(samples_truth.data[\"redshift\"][i], c=\"k\", ls=\"--\")\n",
    "\n",
    "    # remove x-ticks on top row\n",
    "    if i < 3:\n",
    "        ax.set(xticks=[])\n",
    "    # set x-label on bottom row\n",
    "    else:\n",
    "        ax.set(xlabel=\"redshift\")\n",
    "    # set y-label on far left column\n",
    "    if i % 3 == 0:\n",
    "        ax.set(ylabel=\"p(z)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true posteriors are in blue, and the true redshifts are marked by the vertical black lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ErrConv\"></a>\n",
    "## 2. Calculating posteriors while convolving errors\n",
    "Now, let's get a little more sophisticated.\n",
    "\n",
    "Let's recreate the Engine/Degredation we were using at the end of the Degradation demo. \n",
    "\n",
    "I will make one change however:\n",
    "the LSST Error Model sometimes results in non-detections for faint galaxies.\n",
    "These non-detections are flagged with inf.\n",
    "Calculating posteriors for galaxies with non-detections is more complicated, so for now, I will add one additional QuantityCut to remove any galaxies with missing magnitudes.\n",
    "To see how to calculate posteriors for galaxies with missing magnitudes, see [Section 3](#MissingBands)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's draw a degraded sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the error model\n",
    "\n",
    "n_samples = 30\n",
    "# create the FlowEngine\n",
    "flowEngine_degr = FlowCreator.make_stage(\n",
    "    name=\"degraded\", flow_file=flow_file, n_samples=n_samples\n",
    ")\n",
    "# draw a few samples\n",
    "samples_degr = flowEngine_degr.sample(n_samples, seed=0)\n",
    "errorModel = LSSTErrorModel.make_stage(name=\"lsst_errors\", input=\"xx\", sigLim=5)\n",
    "quantityCut = QuantityCut.make_stage(\n",
    "    name=\"gold_cut\", input=\"xx\", cuts={band: np.inf for band in \"ugrizy\"}\n",
    ")\n",
    "inv_incomplete = InvRedshiftIncompleteness.make_stage(\n",
    "    name=\"incompleteness\", pivot_redshift=0.8\n",
    ")\n",
    "\n",
    "OII = 3727\n",
    "OIII = 5007\n",
    "\n",
    "lc_2p_0II_0III = LineConfusion.make_stage(\n",
    "    name=\"lc_2p_0II_0III\", true_wavelen=OII, wrong_wavelen=OIII, frac_wrong=0.02\n",
    ")\n",
    "lc_1p_0III_0II = LineConfusion.make_stage(\n",
    "    name=\"lc_1p_0III_0II\", true_wavelen=OIII, wrong_wavelen=OII, frac_wrong=0.01\n",
    ")\n",
    "detection = QuantityCut.make_stage(name=\"detection\", cuts={\"i\": 25.3})\n",
    "\n",
    "data = samples_degr\n",
    "for degr in [\n",
    "    errorModel,\n",
    "    quantityCut,\n",
    "    inv_incomplete,\n",
    "    lc_2p_0II_0III,\n",
    "    lc_1p_0III_0II,\n",
    "    detection,\n",
    "]:\n",
    "    data = degr(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_degraded_wo_nondetects = data.data\n",
    "samples_degraded_wo_nondetects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample has photometric errors that we would like to convolve in the redshift posteriors, so that the posteriors are fully consistent with the errors. We can perform this convolution by sampling from the error distributions, calculating posteriors, and averaging.\n",
    "\n",
    "`FlowEngine` has this functionality already built in - we just have to provide `err_samples` to the `get_posterior` method.\n",
    "\n",
    "Let's calculate posteriors with a variable number of error samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(0, 2.5, 100)\n",
    "\n",
    "\n",
    "def get_degr_post(key, data, **kwargs):\n",
    "    flow_degr_post = FlowPosterior.make_stage(name=f\"degr_post_{key}\", **kwargs)\n",
    "    return flow_degr_post.get_posterior(data, column=\"redshift\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degr_kwargs = dict(\n",
    "    column=\"redshift\",\n",
    "    flow_file=flow_file,\n",
    "    marg_rules=dict(flag=np.nan, u=lambda row: np.linspace(25, 31, 10)),\n",
    "    grid=grid,\n",
    "    seed=0,\n",
    "    batch_size=2,\n",
    ")\n",
    "pdfs_errs_convolved = {\n",
    "    err_samples: get_degr_post(\n",
    "        f\"{str(err_samples)}\", data, err_samples=err_samples, **degr_kwargs\n",
    "    )\n",
    "    for err_samples in [1, 10, 100, 1000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, dpi=120)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # set dummy values for xlim\n",
    "    xlim = [np.inf, -np.inf]\n",
    "\n",
    "    for pdfs_ in pdfs_errs_convolved.values():\n",
    "        # plot the pdf\n",
    "        pdfs_.data[i].plot_native(axes=ax)\n",
    "\n",
    "        # get the x value where the pdf first rises above 2\n",
    "        xmin = grid[np.argmax(pdfs_.data[i].pdf(grid)[0] > 2)]\n",
    "        if xmin < xlim[0]:\n",
    "            xlim[0] = xmin\n",
    "\n",
    "        # get the x value where the pdf finally falls below 2\n",
    "        xmax = grid[-np.argmax(pdfs_.data[i].pdf(grid)[0, ::-1] > 2)]\n",
    "        if xmax > xlim[1]:\n",
    "            xlim[1] = xmax\n",
    "\n",
    "    # plot the true redshift\n",
    "    z_true = samples_degraded_wo_nondetects[\"redshift\"].iloc[i]\n",
    "    ax.axvline(z_true, c=\"k\", ls=\"--\")\n",
    "\n",
    "    # set x-label on bottom row\n",
    "    if i >= 3:\n",
    "        ax.set(xlabel=\"redshift\")\n",
    "    # set y-label on far left column\n",
    "    if i % 3 == 0:\n",
    "        ax.set(ylabel=\"p(z)\")\n",
    "\n",
    "    # set the x-limits so we can see more detail\n",
    "    xlim[0] -= 0.2\n",
    "    xlim[1] += 0.2\n",
    "    ax.set(xlim=xlim, yticks=[])\n",
    "\n",
    "# create the legend\n",
    "axes[0, 1].plot([], [], c=\"C0\", label=f\"1 sample\")\n",
    "for i, n in enumerate([10, 100, 1000]):\n",
    "    axes[0, 1].plot([], [], c=f\"C{i+1}\", label=f\"{n} samples\")\n",
    "axes[0, 1].legend(\n",
    "    bbox_to_anchor=(0.5, 1.3),\n",
    "    loc=\"upper center\",\n",
    "    ncol=4,\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the effect of convolving the errors. In particular, notice that without error convolution (1 sample), the redshift posterior is often totally inconsistent with the true redshift (marked by the vertical black line). As you convolve more samples, the posterior generally broadens and becomes consistent with the true redshift.\n",
    "\n",
    "Also notice how the posterior continues to change as you convolve more and more samples. This suggests that you need to do a little testing to ensure that you have convolved enough samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating posteriors with missing bands\n",
    "\n",
    "Now let's finally tackle posterior calculation with missing bands.\n",
    "\n",
    "First, lets make a sample that has missing bands. Let's use the same degrader as we used above, except without the final QuantityCut that removed non-detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_degraded = DS[\"output_lc_1p_0III_0II\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that galaxy 3 has a non-detection in the u band. `FlowEngine` can handle missing values by marginalizing over that value. By default, `FlowEngine` will marginalize over NaNs in the u band, using the grid `u = np.linspace(25, 31, 10)`. This default grid should work in most cases, but you may want to change the flag for non-detections, use a different grid for the u band, or marginalize over non-detections in other bands. In order to do these things, you must supply `FlowEngine` with marginalization rules in the form of the `marg_rules` dictionary.\n",
    "\n",
    "Let's imagine we want to use a different grid for u band marginalization. In order to determine what grid to use, we will create a histogram of non-detections in u band vs true u band magnitude (assuming year 10 LSST errors). This will tell me what are reasonable values of u to marginalize over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get true u band magnitudes\n",
    "true_u = DS[\"output_degraded\"].data[\"u\"].to_numpy()\n",
    "# get the observed u band magnitudes\n",
    "obs_u = DS[\"output_lsst_errors\"].data[\"u\"].to_numpy()\n",
    "\n",
    "# create the figure\n",
    "fig, ax = plt.subplots(constrained_layout=True, dpi=100)\n",
    "# plot the u band detections\n",
    "ax.hist(true_u[np.isfinite(obs_u)], bins=10, range=(23, 31), label=\"detected\")\n",
    "# plot the u band non-detections\n",
    "ax.hist(true_u[~np.isfinite(obs_u)], bins=10, range=(23, 31), label=\"non-detected\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"true u magnitude\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this histogram, I will marginalize over u band values from 27 to 31. Like how I tested different numbers of error samples above, here I will test different resolutions for the u band grid.\n",
    "\n",
    "I will provide our new u band grid in the `marg_rules` dictionary, which will also include `\"flag\"` which tells `FlowEngine` what my flag for non-detections is.\n",
    "In this simple example, we are using a fixed grid for the u band, but notice that the u band rule takes the form of a function - this is because the grid over which to marginalize can be a function of any of the other variables in the row. \n",
    "If I wanted to marginalize over any other bands, I would need to include corresponding rules in `marg_rules` too.\n",
    "\n",
    "For this example, I will only calculate pdfs for galaxy 3, which is the galaxy with a non-detection in the u band. Also, similarly to how I tested the error convolution with a variable number of samples, I will test the marginalization with varying resolutions for the marginalized grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core.util_stages import RowSelector\n",
    "\n",
    "# dict to save the marginalized posteriors\n",
    "pdfs_u_marginalized = {}\n",
    "\n",
    "row3_selector = RowSelector.make_stage(name=\"select_row3\", start=3, stop=4)\n",
    "row3_degraded = row3_selector(samples_degraded)\n",
    "\n",
    "degr_post_kwargs = dict(\n",
    "    grid=grid, err_samples=10000, seed=0, flow_file=flow_file, column=\"redshift\"\n",
    ")\n",
    "\n",
    "# iterate over variable grid resolution\n",
    "for nbins in [10, 20, 50, 100]:\n",
    "    # set up the marginalization rules for this grid resolution\n",
    "    marg_rules = {\n",
    "        \"flag\": errorModel.config[\"ndFlag\"],\n",
    "        \"u\": lambda row: np.linspace(27, 31, nbins),\n",
    "    }\n",
    "\n",
    "    # calculate the posterior by marginalizing over u and sampling\n",
    "    # from the error distributions of the other galaxies\n",
    "    pdfs_u_marginalized[nbins] = get_degr_post(\n",
    "        f\"degr_post_nbins_{nbins}\",\n",
    "        row3_degraded,\n",
    "        marg_rules=marg_rules,\n",
    "        **degr_post_kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "for i in [10, 20, 50, 100]:\n",
    "    pdfs_u_marginalized[i]()[0].plot_native(axes=ax, label=f\"{i} bins\")\n",
    "ax.axvline(samples_degraded().iloc[3][\"redshift\"], label=\"True redshift\", c=\"k\")\n",
    "ax.legend()\n",
    "ax.set(xlabel=\"Redshift\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resolution with only 10 bins is sufficient for this marginalization.\n",
    "\n",
    "In this example, only one of the bands featured a non-detection, but you can easily marginalize over more bands by including corresponding rules in the `marg_rules` dict.  Note that marginalizing over multiple bands quickly gets expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cc5b75da6b94fd59f6c7b0992047078a9372d2242c3f23a554e39af5a039534"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
