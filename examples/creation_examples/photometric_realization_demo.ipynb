{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometric Realization from Different Magnitude Error Models\n",
    "\n",
    "author: John Franklin Crenshaw, Sam Schmidt, Eric Charles, Ziang Yan\n",
    "\n",
    "last run successfully: August 2, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to do photometric realization from different magnitude error models. For more completed degrader demo, see `degradation-demo.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pzflow.examples import get_example_flow\n",
    "from rail.creation.engines.flowEngine import FlowCreator\n",
    "from rail.creation.degradation.lsst_error_model import LSSTErrorModel\n",
    "from rail.core.stage import RailStage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path to the pretrained 'pzflow' used to generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pzflow\n",
    "import os\n",
    "\n",
    "flow_file = os.path.join(\n",
    "    os.path.dirname(pzflow.__file__), \"example_files\", \"example-flow.pzflow.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by setting up the RAIL data store.  RAIL uses [ceci](https://github.com/LSSTDESC/ceci), which is designed for pipelines rather than interactive notebooks, the data store will work around that and enable us to use data interactively.  See the `rail/examples/goldenspike_examples/goldenspike.ipynb` example notebook for more details on the Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"True\" Engine\n",
    "\n",
    "First, let's make an Engine that has no degradation. We can use it to generate a \"true\" sample, to which we can compare all the degraded samples below.\n",
    "\n",
    "Note: in this example, we will use a normalizing flow engine from the [pzflow](https://github.com/jfcrenshaw/pzflow) package. However, everything in this notebook is totally agnostic to what the underlying engine is.\n",
    "\n",
    "The Engine is a type of RailStage object, so we can make one using the `RailStage.make_stage` function for the class of Engine that we want.  We then pass in the configuration parameters as arguments to `make_stage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(1e5)\n",
    "flowEngine_truth = FlowCreator.make_stage(\n",
    "    name=\"truth\", model=flow_file, n_samples=n_samples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the Engine correctly read the underlying PZ Flow object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowEngine_truth.get_data(\"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we invoke the `sample` method to generate some samples\n",
    "\n",
    "Note that this will return a `DataHandle` object, which can keep both the data itself, and also the path to where the data is written.  When talking to rail stages we can use this as though it were the underlying data and pass it as an argument.  This allows the rail stages to keep track of where their inputs are coming from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate magnitude error for extended sources, we need the information about major and minor axes of each galaxy. Here we simply generate random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_truth = flowEngine_truth.sample(n_samples, seed=0)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "samples_truth.data[\"major\"] = np.abs(\n",
    "    np.random.normal(loc=0.01, scale=0.1, size=n_samples)\n",
    ")  # add major and minor axes\n",
    "b_to_a = 1 - 0.5 * np.random.rand(n_samples)\n",
    "samples_truth.data[\"minor\"] = samples_truth.data[\"major\"] * b_to_a\n",
    "\n",
    "print(samples_truth())\n",
    "print(\"Data was written to \", samples_truth.path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### LSSTErrorModel\n",
    "\n",
    "Now, we will demonstrate the `LSSTErrorModel`, which adds photometric errors using a model similar to the model from [Ivezic et al. 2019](https://arxiv.org/abs/0805.2366) (specifically, it uses the model from this paper, without making the high SNR assumption. To restore this assumption and therefore use the exact model from the paper, set `highSNR=True`.)\n",
    "\n",
    "Let's create an error model with the default settings for point sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel = LSSTErrorModel.make_stage(name=\"error_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extended sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel_auto = LSSTErrorModel.make_stage(\n",
    "    name=\"error_model_auto\", extendedType=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorModel_gaap = LSSTErrorModel.make_stage(\n",
    "    name=\"error_model_gaap\", extendedType=\"gaap\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add this error model as a degrader and draw some samples with photometric errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs = errorModel(samples_truth)\n",
    "samples_w_errs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs_gaap = errorModel_gaap(samples_truth)\n",
    "samples_w_errs_gaap.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_w_errs_auto = errorModel_auto(samples_truth)\n",
    "samples_w_errs_auto.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice some of the magnitudes are inf's.\n",
    "These are non-detections (i.e. the noisy flux was negative).\n",
    "You can change the nSigma limit for non-detections by setting `sigLim=...`.\n",
    "For example, if `sigLim=5`, then all fluxes with `SNR<5` are flagged as non-detections.\n",
    "\n",
    "Let's plot the error as a function of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes_ = plt.subplots(ncols=3, nrows=2, figsize=(15, 9), dpi=100)\n",
    "axes = axes_.reshape(-1)\n",
    "for i, band in enumerate(\"ugrizy\"):\n",
    "    ax = axes[i]\n",
    "    # pull out the magnitudes and errors\n",
    "    mags = samples_w_errs.data[band].to_numpy()\n",
    "    errs = samples_w_errs.data[band + \"_err\"].to_numpy()\n",
    "    \n",
    "    # sort them by magnitude\n",
    "    mags, errs = mags[mags.argsort()], errs[mags.argsort()]\n",
    "    \n",
    "    # plot errs vs mags\n",
    "    #ax.plot(mags, errs, label=band) \n",
    "    \n",
    "    #plt.plot(mags, errs, c='C'+str(i))\n",
    "    ax.scatter(samples_w_errs_gaap.data[band].to_numpy(),\n",
    "            samples_w_errs_gaap.data[band + \"_err\"].to_numpy(),\n",
    "                s=5, marker='.', color='C0', alpha=0.8, label='GAAP')\n",
    "    \n",
    "    ax.plot(mags, errs, color='C3', label='Point source')\n",
    "    \n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlim(18, 31)\n",
    "    ax.set_ylim(-0.1, 3.5)\n",
    "    ax.set(xlabel=band+\" Band Magnitude (AB)\", ylabel=\"Error (mags)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes_ = plt.subplots(ncols=3, nrows=2, figsize=(15, 9), dpi=100)\n",
    "axes = axes_.reshape(-1)\n",
    "for i, band in enumerate(\"ugrizy\"):\n",
    "    ax = axes[i]\n",
    "    # pull out the magnitudes and errors\n",
    "    mags = samples_w_errs.data[band].to_numpy()\n",
    "    errs = samples_w_errs.data[band + \"_err\"].to_numpy()\n",
    "    \n",
    "    # sort them by magnitude\n",
    "    mags, errs = mags[mags.argsort()], errs[mags.argsort()]\n",
    "    \n",
    "    # plot errs vs mags\n",
    "    #ax.plot(mags, errs, label=band) \n",
    "    \n",
    "    #plt.plot(mags, errs, c='C'+str(i))\n",
    "    ax.scatter(samples_w_errs_auto.data[band].to_numpy(),\n",
    "            samples_w_errs_auto.data[band + \"_err\"].to_numpy(),\n",
    "                s=5, marker='.', color='C0', alpha=0.8, label='AUTO')\n",
    "    \n",
    "    ax.plot(mags, errs, color='C3', label='Point source')\n",
    "    \n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlim(18, 31)\n",
    "    ax.set_ylim(-0.1, 3.5)\n",
    "    ax.set(xlabel=band+\" Band Magnitude (AB)\", ylabel=\"Error (mags)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the photometric error increases as magnitude gets dimmer, just like you would expect, and that the extended source errors are greater than the point source errors.\n",
    "The extended source errors are also scattered, because the galaxies have random sizes.\n",
    "\n",
    "Also, you can find the GAaP and AUTO magnitude error are scattered due to variable galaxy sizes. Also, you can find that there are gaps between GAAP magnitude error and point souce magnitude error, this is because the additional factors due to aperture sizes have a minimum value of $\\sqrt{(\\sigma^2+A_{\\mathrm{min}})/\\sigma^2}$, where $\\sigma$ is the width of the beam, $A_{\\min}$ is an offset of the aperture sizes (taken to be 0.7 arcmin here).\n",
    "\n",
    "You can also see that there are *very* faint galaxies in this sample.\n",
    "That's because, by default, the error model returns magnitudes for all positive fluxes.\n",
    "If you want these galaxies flagged as non-detections instead, you can set e.g. `sigLim=5`, and everything with `SNR<5` will be flagged as a non-detection."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33169e9e67880b1853f4eac3eefbf27d2cf9aef466064efe02e233d73165a438"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
