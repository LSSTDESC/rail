{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering redshifts with *yet_another_wizz*\n",
    "\n",
    "This notebooks summarises the steps to compute clustering redshifts for an\n",
    "unknown sample of galaxies using a reference sample with known redshifts.\n",
    "Additionally, a correction for the galaxy bias of the reference sample is\n",
    "applied (see Eqs. 17 & 20 in\n",
    "[van den Busch et al. 2020](https://arxiv.org/pdf/2007.01846)).\n",
    "\n",
    "This involves a number of steps (see schema below):\n",
    "1. Preparing the input data (creating randoms, applying masks; simplfied here).\n",
    "2. Splitting the data into spatial patches and cache them on disk for faster access.\n",
    "3. Computing the autocorrelation function amplitude $w_{\\rm ss}(z)$, used as\n",
    "   correction for the galaxy bias\n",
    "4. Computing the cross-correlation function amplitude $w_{\\rm sp}(z)$, which is\n",
    "   the biased redshift estimate.\n",
    "5. Summarising the result by correcting for the refernece sample bias and\n",
    "   producing a redshift estimate (not a PDF!).\n",
    "\n",
    "**Note:** The cached data must be removed manually since its lifetime can currently\n",
    "not be handled by RAIL.\n",
    "\n",
    "The aim of this notebook is to **give an overview of the wrapper functionality**,\n",
    "including a summary of all currently implemented optional parameters (commented).\n",
    "It is not meant to be a demonstaration of the performance of *yet_another_wizz*\n",
    "since the example data used here is very small and the resulting signal-to-noise\n",
    "ratio is quite poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rail_yaw_network.png\" alt=\"RAIL YAW stage network\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rail.core.data import TableHandle\n",
    "\n",
    "# configure RAIL datastore to allow overwriting data\n",
    "from rail.core.stage import RailStage\n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1\n",
    "import pandas as pd\n",
    "from yaw.randoms import BoxRandoms\n",
    "from rail.yaw_rail.utils import get_dc2_test_data\n",
    "\n",
    "from rail.estimation.algos.cc_yaw import (\n",
    "    create_yaw_cache_alias,  # utility for YawCacheCreate\n",
    "    YawCacheCreate,     # step 2\n",
    "    YawAutoCorrelate,   # step 3\n",
    "    YawCrossCorrelate,  # step 4\n",
    "    YawSummarize,       # step 5\n",
    ")  # equivalent: from rail.yaw_rail import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = \"debug\"  # verbosity level of built-in logger, disable with \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "Since this is a simple example, we are not going into the details of creating\n",
    "realistic randoms and properly masking the reference and unknown data to a\n",
    "shared footprint on sky. Instead, we are using a simulated dataset that serves\n",
    "as both, reference and unknown sample.\n",
    "\n",
    "First, we download the small test dataset derived from 25 sqdeg of DC2,\n",
    "containing 100k objects on a limited redshift range of $0.2 < z < 1.8$. We add\n",
    "the data as new handle to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_dc2_test_data()  # downloads test data, cached for future calls\n",
    "redshifts = test_data[\"z\"].to_numpy()\n",
    "zmin = redshifts.min()\n",
    "zmax = redshifts.max()\n",
    "n_data = len(test_data)\n",
    "print(f\"N={n_data}, {zmin:.1f}<z<{zmax:.1f}\")\n",
    "\n",
    "handle_test_data = DS.add_data(\"input_data\", test_data, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate a x10 enhanced uniform random dataset for the test data\n",
    "constrained to its rectangular footprint. We add redshifts by cloning the\n",
    "redshift column `\"z\"` of the dataset.  We add the randoms as new handle to the\n",
    "datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BoxRandoms(\n",
    "    test_data[\"ra\"].min(),\n",
    "    test_data[\"ra\"].max(),\n",
    "    test_data[\"dec\"].min(),\n",
    "    test_data[\"dec\"].max(),\n",
    "    redshifts=redshifts,\n",
    "    seed=12345,\n",
    ")\n",
    "test_rand = generator.generate_dataframe(n_data * 10)\n",
    "test_rand.rename(columns=dict(redshifts=\"z\"), inplace=True)\n",
    "\n",
    "handle_test_rand = DS.add_data(\"input_rand\", test_rand, TableHandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting and caching the data\n",
    "\n",
    "This step is crucial to compute consistent clustering redshift uncertainties.\n",
    "*yet_another_wizz* uses spatial (jackknife) resampling and therefore, every\n",
    "input dataset must be split into the same exact spatial regions/patches. To\n",
    "improve the parallel performance, the datasets and randoms are pre-arranged into\n",
    "these patches and cached on disk for better random patch-wise access. While this\n",
    "is slow for small datasets, it is highly beneficial for large datasets with many\n",
    "patches and/or memory constraints.\n",
    "\n",
    "The RAIL wrapper uses manually specified cache directories, each of which contains\n",
    "one dataset and optionally corresponding randoms. This ensures that the patch\n",
    "centers are defined consistently. To create a new cache, use the\n",
    "`YawCacheCreate.create()` method.\n",
    "\n",
    "### Note on names and aliasing in RAIL\n",
    "\n",
    "We need to create separate caches for the reference and the unknown data, which\n",
    "means that we need to run the `YawCacheCreate` twice. Since that creates name\n",
    "clashes in the RAIL datastore, we need to properly alias the inputs (`data`/\n",
    "`rand`) and the output (`cache`) by providing a dictionary for the `aliases`\n",
    "parameter when calling the `make_stage()`, e.g. by adding a unique suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"stage_name\"\n",
    "aliases = dict(data=\"data_suffix\", rand=\"rand_suffix\", cache=\"cache_suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a shorthand for convenience (`from rail.yaw_rail.cache.AliasHelper`)\n",
    "that allows to generate this dictionary by just providing a suffix name for the\n",
    "stage instance (see example below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"stage_name\"\n",
    "aliases = create_yaw_cache_alias(\"suffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reference data\n",
    "\n",
    "To create a cache directory we must specify a `path` to the directory at which the\n",
    "data will be cached. This directory must not exist yet. We also have to specify\n",
    "a `name` for the stage to ensure that the reference and unknown caches (see below)\n",
    "are properly aliased to be distinguishable by the RAIL datastore.\n",
    "\n",
    "Furthermore, a few basic column names that describe the tabular input data must\n",
    "be provided. These are right ascension (`ra_name`) and declination (`dec_name`),\n",
    "and in case of the reference sample also the redshifts (`redshift_name`).\n",
    "Finally, the patches must be defined and there are three ways to do so:\n",
    "1. Stage parameter `patch_file`: Read the patch center coordinates from an\n",
    "   ASCII file with pairs of R.A/Dec. coordinates in radian.\n",
    "2. Stage parameter `patch_num`: Generating a given number of patches from the\n",
    "   object positions (peferrably of the randoms if possible) using k-means \n",
    "   clustering.\n",
    "3. Stage parameter `patch_name`: Providing a column name in the input table\n",
    "   which contains patch indices (using 0-based indexing).\n",
    "4. Stage input `patch_source`: Using the patch centers from a different cache\n",
    "   instance, given by a cache handle. When this input is provided it takes\n",
    "   precedence over any of the stage parameters above.\n",
    "\n",
    "In this example we choose to auto-generate five patches. **In a more realistic\n",
    "setup this number should be much larger**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cache_ref = YawCacheCreate.make_stage(\n",
    "    name=\"cache_ref\",\n",
    "    aliases=create_yaw_cache_alias(\"ref\"),\n",
    "    path=\"run/test_ref\",\n",
    "    overwrite=True,  # default: False\n",
    "    ra_name=\"ra\",\n",
    "    dec_name=\"dec\",\n",
    "    redshift_name=\"z\",\n",
    "    # weight_name=None,\n",
    "    # patch_name=None,\n",
    "    patch_num=5,  # default: None\n",
    "    # max_workers=None,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "handle_cache_ref = stage_cache_ref.create(\n",
    "    data=handle_test_data,\n",
    "    rand=handle_test_rand,\n",
    "    # patch_source=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the log messages that *yet_another_wizz* processes the randoms\n",
    "first and generates patch centers (`creating 5 patches`) and then applies them\n",
    "to the dataset, which is processed last (`applying 5 patches`). Caching the data\n",
    "can take considerable time depending on the hardware and the number of patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The unknown data\n",
    "\n",
    "The same procedure for the unknown sample, however there are some small, but\n",
    "important differences. We use a different `path` and `name`, do not specify the\n",
    "`redshift_name` (since we would not have this information with real data), and\n",
    "here we chose to not provide any randoms for the unknown sample and instead rely\n",
    "on the reference sample randoms for cross-correlation measurements.\n",
    "\n",
    "Most importantly, we must ensure that the patch centers are consistent with the\n",
    "reference data and therefore provide the reference sample cache as a stage input\n",
    "called `patch_source`.\n",
    "\n",
    "**Important:** Even if the reference and unknown data are the same as in this\n",
    "specific case, the automatically generated patch centers are not deterministic.\n",
    "We can see in the log messages that the code reports `applying 5 patches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cache_unk = YawCacheCreate.make_stage(\n",
    "    name=\"cache_unk\",\n",
    "    aliases=create_yaw_cache_alias(\"unk\"),\n",
    "    path=\"run/test_unk\",\n",
    "    overwrite=True,  # default: False\n",
    "    ra_name=\"ra\",\n",
    "    dec_name=\"dec\",\n",
    "    # redshift_name=None,\n",
    "    # weight_name=None,\n",
    "    # patch_name=None,\n",
    "    # patch_num=None,\n",
    "    # max_workers=None,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "handle_cache_unk = stage_cache_unk.create(\n",
    "    data=handle_test_data,\n",
    "    # rand=None,\n",
    "    patch_source=handle_cache_ref,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing the autocorrelation / bias correction\n",
    "\n",
    "The bias correction is computed from the amplitude of the angular autocorrelation\n",
    "function of the reference sample. The measurement parameters are the same as for\n",
    "the cross-correlation amplitude measurement, so we can define all configuration\n",
    "parameters once in a dictionary.\n",
    "\n",
    "As a first step, we need to decide on which redshift bins/sampling we want to\n",
    "compute the clustering redshifts. Here we choose the redshift limits of the\n",
    "reference data (`zmin`/`zmax`) and, since the sample is small, only 8 bins\n",
    "(`zbin_num`) spaced linearly in redshift (default `method=\"linear\"`). Finally,\n",
    "we have to define the physical scales in kpc (`rmin`/`rmax`, converted to\n",
    "angular separation at each redshift) on which we measure the correlation\n",
    "amplitudes.\n",
    "\n",
    "**Optional parameters:** Bins edges can alternatively specifed manually through\n",
    "`zbins`. To apply scale dependent weights, e.g. $w \\propto r^{-1}$, specify\n",
    "the power-law exponent as`rweight=-1`. The parameter `resolution` specifies the\n",
    "radial resolution (logarithmic) of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_config = dict(\n",
    "    rmin=100,   # in kpc\n",
    "    rmax=1000,  # in kpc\n",
    "    # rweight=None,\n",
    "    # resolution=50,\n",
    "    zmin=zmin,\n",
    "    zmax=zmax,\n",
    "    num_bins=8,  # default: 30\n",
    "    # method=\"linear\",\n",
    "    # edges=np.linspace(zmin, zmax, zbin_num+1)\n",
    "    # closed=\"right\",\n",
    "    # max_workers=None,\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then measure the autocorrelation using the `YawAutoCorrelate.correlate()`\n",
    "method, which takes a single parameter, the cache (handle) of the reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_auto_corr = YawAutoCorrelate.make_stage(\n",
    "    name=\"auto_corr\",\n",
    "    **corr_config,\n",
    ")\n",
    "handle_auto_corr = stage_auto_corr.correlate(\n",
    "    sample=handle_cache_ref,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the code is progressing, we can observe the log messages of *yet_another_wizz*\n",
    "which indicate the performed steps: getting the cached data, generating the\n",
    "job list of patches to correlate, and counting pairs. Finally, the pair counts\n",
    "are stored as custom data handle in the datastore.\n",
    "\n",
    "We can interact with the returned pair counts (`yaw.CorrFunc`,\n",
    "[documentation](https://yet-another-wizz.readthedocs.io/en/latest/api/correlation/yaw.correlation.CorrFunc.html))\n",
    "manually if we want to investigate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_auto = handle_auto_corr.data  # extract payload from handle\n",
    "counts_auto.dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing the cross-correlation / redshift estimate\n",
    "\n",
    "The cross-correlation amplitude, which is the biased estimate of the unknown\n",
    "redshift distribution, is computed similarly to the autocorrelation above. We\n",
    "measure the correlation using the `YawCrossCorrelate.correlate()` method, which\n",
    "takes two parameters, the cache (handles) of the reference and the unknown data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_cross_corr = YawCrossCorrelate.make_stage(\n",
    "    name=\"cross_corr\",\n",
    "    **corr_config,\n",
    ")\n",
    "handle_cross_corr = stage_cross_corr.correlate(\n",
    "    reference=handle_cache_ref,\n",
    "    unknown=handle_cache_unk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see the actions performed by *yet_another_wizz*. The main\n",
    "difference for the cross-correlation function is that the second sample (the\n",
    "unknown data/randoms) are not binned by redshift when counting pairs.\n",
    "\n",
    "As for the autocorrelation, we can interact with the result, e.g. by evaluating\n",
    "the correlation estimator manually and getting the cross-correlation amplitude\n",
    "per redshift bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_cross = handle_cross_corr.data  # extract payload from handle\n",
    "corrfunc = counts_cross.sample()  # evaluate the correlation estimator\n",
    "corrfunc.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the redshift estimate\n",
    "\n",
    "The final analysis step is combining the two measured correlation amplitudes to\n",
    "get a redshift estimate which is corrected for the reference sample bias.\n",
    "**This estimate is not a PDF.** Converting the result to a proper PDF (without\n",
    "negative values) is non-trivial and requires further modelling stages that are\n",
    "currently not part of this wrapper.\n",
    "\n",
    "We use `YawSummarize.summarize()` method, which takes the pair count handles of\n",
    "the cross- and autocorrelation functions as input. In principle, the\n",
    "autocorrelation of the unknown sample could be specified to fully correct for\n",
    "galaxy bias, however this is not possible in practice since the exact redshifts\n",
    "of the unknown objects are not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_summarize = YawSummarize.make_stage(\n",
    "    name=\"summarize\",\n",
    "    verbose=VERBOSE,  # default: \"info\"\n",
    ")\n",
    "handle_summarize = stage_summarize.summarize(\n",
    "    cross_corr=handle_cross_corr,\n",
    "    auto_corr_ref=handle_auto_corr,  # default: None\n",
    "    # auto_corr_unk=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stage produces a single output which contains the redshift estimate with\n",
    "uncertainties, jackknife samples of the estimate, and a covariance matrix. These\n",
    "data products are wrapped as `yaw.RedshiftData`\n",
    "[documentation](https://yet-another-wizz.readthedocs.io/en/latest/api/redshifts/yaw.redshifts.RedshiftData.html#yaw.redshifts.RedshiftData)\n",
    "which gets stored as `pickle` file when running a `ceci` pipeline. Some examples\n",
    "on how to use this data is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove caches\n",
    "\n",
    "The cached datasets are not automatically removed, since the algorithm does not\n",
    "know when they are no longer needed. Additionally, the reference data could be\n",
    "resued for future runs, e.g. for different tomographic bins.\n",
    "\n",
    "Since that is not the case here, we just delete the cached data with a built-in\n",
    "method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_cache_ref.data.drop()\n",
    "handle_cache_unk.data.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect results\n",
    "\n",
    "Below are some examples on how to access the redshift binning, estimate,\n",
    "estimte error, samples and covariance matrix produced by *yet_another_wizz*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc = handle_summarize.data\n",
    "ncc.data / ncc.error  # n redshift slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true n(z)\n",
    "zbins = handle_cross_corr.data.binning.edges\n",
    "plt.hist(test_data[\"z\"], zbins, density=True, color=\"0.8\", label=\"true n(z)\")\n",
    "\n",
    "# fiducial n(z)\n",
    "normalised = ncc.normalised()  # copy of data with n(z) is normalised to unity\n",
    "ax = normalised.plot(label=\"YAW estimate\")\n",
    "\n",
    "# jackknife samples\n",
    "normalised.samples.shape  # m jackknife-samples x n redshift slices\n",
    "z = normalised.binning.mids\n",
    "plt.plot(z, normalised.samples.T, color=\"k\", alpha=0.2)\n",
    "# create a dummy for the legend\n",
    "plt.plot([], [], color=\"k\", alpha=0.2, label=\"jackknife samples\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncc.covariance.shape  # n x n redshift slices\n",
    "ncc.plot_corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail_yaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
