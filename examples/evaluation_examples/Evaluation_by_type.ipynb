{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: RAIL Evaluation \n",
    "\n",
    "**Authors:** Drew Oldag, Eric Charles, Sam Schmidt, Alex Malz, Julia Gschwend, others...\n",
    "\n",
    "**Last run successfully:** April 10, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to demonstrate the application of the metrics scripts to be used on the photo-z PDF catalogs produced by the PZ working group. The first implementation of the _evaluation_ module is based on the refactoring of the code used in [Schmidt et al. 2020](https://arxiv.org/pdf/2001.03621.pdf), available on Github repository [PZDC1paper](https://github.com/LSSTDESC/PZDC1paper). \n",
    "\n",
    "To run this notebook, you must install qp and have the notebook in the same directory as `utils.py` (available in RAIL's examples directrory). You must also have installed all RAIL dependencies, particularly for the estimation codes that you want to run, as well as ceci, qp, tables_io, etc...  See the RAIL installation instructions for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables_io\n",
    "import qp\n",
    "import numpy as np\n",
    "\n",
    "from rail.evaluation.dist_to_dist_evaluator import DistToDistEvaluator\n",
    "from rail.evaluation.dist_to_point_evaluator import DistToPointEvaluator\n",
    "from rail.evaluation.point_to_point_evaluator import PointToPointEvaluator\n",
    "from rail.evaluation.single_evaluator import SingleEvaluator\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.data import QPHandle, TableHandle, QPOrTableHandle\n",
    "\n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load example Data\n",
    "\n",
    "This will load (and download if needed) two files:\n",
    "\n",
    "1. output_fzboost.hdf5: a `qp` ensemble with the output of running the FZBoost alogrithm to estimate redshifts\n",
    "2. test_dc2_validation_9816.hdf5: a `hdf5` file with a table with the photometric data used to generate the first file\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rail.utils.path_utils import find_rail_file\n",
    "possible_local_file = './examples_data/evaluation_data/data/output_fzboost.hdf5'\n",
    "if os.path.exists(possible_local_file):\n",
    "    pdfs_file = os.path.abspath(possible_local_file)\n",
    "else:\n",
    "    pdfs_file = 'examples_data/evaluation_data/data/output_fzboost.hdf5'\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(pdfs_file))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    curl_com = f\"curl -o {pdfs_file} https://portal.nersc.gov/cfs/lsst/PZ/output_fzboost.hdf5\"\n",
    "    os.system(curl_com)\n",
    "\n",
    "ztrue_file = find_rail_file('examples_data/testdata/test_dc2_validation_9816.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = DS.read_file(key='pdfs_data', handle_class=QPHandle, path=pdfs_file)\n",
    "ztrue_data = DS.read_file('ztrue_data', TableHandle, ztrue_file)\n",
    "#truth = DS.add_data('truth', ztrue_data()['photometry'], TableHandle, path=ztrue_file)\n",
    "#truth_points = DS.add_data('truth_points', ztrue_data()['photometry']['redshift'], TableHandle, path=ztrue_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dist to Dist Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DistToDistEvaluator is for evaluating metrics that compare distributions to distributions.\n",
    "\n",
    "To test it we are going to compare a generated p(z) distribution to itself.\n",
    "\n",
    "Note that there are two modes in which this can be run.   The default mode is to allow evaluation of the metric in parallel across many nodes.   This is much faster, can avoid potential issues with overflowing the memory for huge input data sets, however, when computing quantiles or medians the computation will not be exact (however, with the default parameters it will be very close in almost all cases).  However, if the force_exact configuration variable is set, it will only run on a single node, allowing for an exact calculation, but with the drawback of being slower and more memory intensive.\n",
    "\n",
    "Here we implement both types, and compare the results.\n",
    "\n",
    "We will run 5 different estimates, follow the links to get more information about each:\n",
    "1. cvm: [Cramer-von Mises](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion)\n",
    "2. ks: [Kolmogorov-Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)\n",
    "3. rmse: [Root-mean-square error](https://en.wikipedia.org/wiki/Root_mean_square)\n",
    "4. kld: [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "5. ad: [Anderson-Darling](https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_dict = dict(\n",
    "    metrics=['cvm', 'ks', 'rmse', 'kld', 'ad'],\n",
    "    _random_state=None,\n",
    ")\n",
    "\n",
    "dtd_stage = DistToDistEvaluator.make_stage(name='dist_to_dist', **stage_dict)\n",
    "dtd_stage_single = DistToDistEvaluator.make_stage(name='dist_to_dist', force_exact=True, **stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelized implementation\n",
    "dtd_results = dtd_stage.evaluate(ensemble, ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-parallelized, exact implementation\n",
    "dtd_results_single = dtd_stage_single.evaluate(ensemble, ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that stage produces a few different outputs:\n",
    "print(dtd_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the output data tables. \n",
    "`dtd_results['output']` returns a DataHandle object, which we can ask for the data themselves, i.e.,\n",
    "`dtd_results['output']()` \n",
    "\n",
    "We use the `tables_io.convertObj` function to convert the output tables to pandas DataFrame objects for better display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = tables_io.convertObj(dtd_results['output'](), tables_io.types.PD_DATAFRAME)\n",
    "results_df_single = tables_io.convertObj(dtd_results_single['output'](), tables_io.types.PD_DATAFRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the table for the parallel processing version\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the table for the exact processing version\n",
    "results_df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dist to Point Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DistToPointEvaluator is for evaluating metrics that compare distributions (for the p(z)) estimate to point values (for the reference or truth).\n",
    "\n",
    "To test it we are going to compare a generated p(z) distribution to true redshifts.\n",
    "\n",
    "Note that as for the DistToDistEvaluator this can be run in parallel or forced to run on a single node for exact results.\n",
    "\n",
    "We will run 3 different estimates, follow the links to get more information about each:\n",
    "1. cdeloss: [Conditional Density Estimation](https://vitaliset.github.io/conditional-density-estimation/)\n",
    "2. pit: [Probability Integral Transform](https://en.wikipedia.org/wiki/Probability_integral_transform)\n",
    "3. brier: [Brier Score](https://en.wikipedia.org/wiki/Brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_dict = dict(\n",
    "    metrics=['cdeloss', 'pit', 'brier'],\n",
    "    _random_state=None,\n",
    "    metric_config={\n",
    "        'brier': {'limits':(0,3.1)},\n",
    "        'pit':{'tdigest_compression': 1000},\n",
    "    }\n",
    ")\n",
    "dtp_stage = DistToPointEvaluator.make_stage(name='dist_to_point', **stage_dict)\n",
    "dtp_stage_single = DistToPointEvaluator.make_stage(name='dist_to_point', force_exact=True, **stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp_results = dtp_stage.evaluate(ensemble, ztrue_data)\n",
    "# The summary results are in a table, which we can convert to a pandas.DataFrame, note that here\n",
    "# we can a single number for the entire ensemble, rather that one number per PDF\n",
    "results_df = tables_io.convertObj(dtp_results['summary'](), tables_io.types.PD_DATAFRAME)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp_results_single = dtp_stage_single.evaluate(ensemble, ztrue_data)\n",
    "results_df_single = tables_io.convertObj(dtp_results_single['summary'](), tables_io.types.PD_DATAFRAME)\n",
    "\n",
    "results_df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another type of output is a distritubion, for example the PIT or probability integral transform\n",
    "dtp_pit = dtp_stage.get_handle('single_distribution_summary').read()['pit']\n",
    "dtp_pit_single = dtp_stage_single.get_handle('single_distribution_summary').read()['pit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgrid = np.linspace(0.05,0.95,100)\n",
    "a_pdf = dtp_pit.pdf(xgrid)\n",
    "b_pdf = dtp_pit_single.pdf(xgrid)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xgrid, np.squeeze(a_pdf), label='parallelized, tdigest approximation')\n",
    "plt.plot(xgrid, np.squeeze(b_pdf), label='non-parallelized, exact')\n",
    "plt.xlabel(\"Quantile\")\n",
    "plt.ylabel(r\"$F_X(X)$ transformation to obtain uniform distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point to Point Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {pomtToPointEvaluator is for evaluating metrics that compare point estimates (for the p(z)) to point values (for the reference or truth).\n",
    "\n",
    "To test it we are going to compare the mode of p(z) distribution to true redshifts.\n",
    "\n",
    "Note that as for the DistToDistEvaluator this can be run in parallel or forced to run on a single node for exact results.\n",
    "\n",
    "\n",
    "We will run 5 different estimates, follow the links to get more information about each:\n",
    "1. point_stats_ez: `(estimate - reference) / (1.0 + reference)`\n",
    "2. point_stats_iqr: 'Interquatile range from 0.25 to 0.75', i.e., the middle 50% of the distribution of point_stats_ez\n",
    "3. point_bias: Median of point_stats_ez\n",
    "4. point_outlier_rate: Fraction of distribution outside of 3 sigma\n",
    "5. point_stats_sigma_mad: Sigma of the median absolute deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_dict = dict(\n",
    "    metrics=['point_stats_ez', 'point_stats_iqr', 'point_bias', 'point_outlier_rate', 'point_stats_sigma_mad'],\n",
    "    _random_state=None,\n",
    "    hdf5_groupname='photometry',\n",
    "    point_estimate_key='zmode',\n",
    "    chunk_size=10000,\n",
    "    metric_config={\n",
    "        'point_stats_iqr':{'tdigest_compression': 100},\n",
    "    }\n",
    ")\n",
    "ptp_stage = PointToPointEvaluator.make_stage(name='point_to_point', **stage_dict)\n",
    "ptp_stage_single = PointToPointEvaluator.make_stage(name='point_to_point', force_exact=True, **stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp_results = ptp_stage.evaluate(ensemble, ztrue_data)\n",
    "results_summary = tables_io.convertObj(ptp_stage.get_handle('summary')(), tables_io.types.PD_DATAFRAME)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp_results_single = ptp_stage_single.evaluate(ensemble, ztrue_data)\n",
    "results_summary_single = tables_io.convertObj(ptp_stage_single.get_handle('summary')(), tables_io.types.PD_DATAFRAME)\n",
    "results_summary_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the effect of the approximation used when running in parallel.  Here we are to do the computation in qp to confirm the exact value is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = ztrue_data()['photometry']['redshift']\n",
    "estimates = np.squeeze(ensemble().ancil['zmode'])\n",
    "#truth_points = DS.add_data('truth_points', ztrue_data()['photometry']['redshift'], TableHandle, path=ztrue_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_iqr = qp.metrics.point_estimate_metric_classes.PointSigmaIQR().evaluate(estimates, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a RailPipeline with an evaluation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'pdfs_data':'examples_data/evaluation_data/data/output_fzboost.hdf5',\n",
    "    'ztrue_data':'examples_data/test_dc2_validation_9816.hdf5',\n",
    "}\n",
    "outputs = {\n",
    "    'output':'output.hdf5',\n",
    "    'summary':'summary.hdf5',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.core import RailPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = RailPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.add_stage(ptp_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.initialize(overall_inputs=inputs, run_config={'output_dir':'.', 'log_dir':'.', 'resume':False}, stages_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.save('eval_pipe.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SingletEvaluator is will computate all of the metrics that it can for the inputs that it is given.\n",
    "\n",
    "It will check to see if the estimate and reference inputs are point estimates or distributions, (or potentially both, e.g., if the use asks to use the mode or median of the distribution as a point estimate.)\n",
    "\n",
    "To test it we are going to compare a generated p(z) distribution to true redshifts.\n",
    "\n",
    "Note that as for the DistToDistEvaluator this can be run in parallel or forced to run on a single node for exact results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_dict = dict(\n",
    "    metrics=['cvm', 'ks', 'omega', 'kld', 'cdeloss', 'point_stats_ez', 'point_stats_iqr'],\n",
    "    _random_state=None,\n",
    "    hdf5_groupname='photometry',\n",
    "    point_estimates=['zmode'],\n",
    "    truth_point_estimates=['redshift'],\n",
    "    chunk_size=1000,\n",
    ")\n",
    "ensemble_d = DS.add_data('pdfs_data_2', None, QPOrTableHandle, path=pdfs_file)\n",
    "ztrue_data_d = DS.add_data('ztrue_data_2', None, QPOrTableHandle, path=ztrue_file)\n",
    "\n",
    "single_stage = SingleEvaluator.make_stage(name='single', **stage_dict)\n",
    "single_stage_single = SingleEvaluator.make_stage(name='single', force_exact=True, **stage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_results = single_stage.evaluate(ensemble_d, ztrue_data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_results_single = single_stage_single.evaluate(ensemble_d, ztrue_data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stage.get_handle('output')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stage.get_handle('summary')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stage_single.get_handle('output')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_stage_single.get_handle('summary')()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF-based Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIT\n",
    "\n",
    "The Probability Integral Transform (PIT), is the Cumulative Distribution Function (CDF) of the photo-z PDF \n",
    "\n",
    "$$ \\mathrm{CDF}(f, q)\\ =\\ \\int_{-\\infty}^{q}\\ f(z)\\ dz $$\n",
    "\n",
    "evaluated at the galaxy's true redshift for every galaxy $i$ in the catalog.\n",
    "\n",
    "$$ \\mathrm{PIT}(p_{i}(z);\\ z_{i})\\ =\\ \\int_{-\\infty}^{z^{true}_{i}}\\ p_{i}(z)\\ dz $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fzdata = DS.read_file('pdfs_data', QPHandle, pdfs_file)\n",
    "ztrue_data = DS.read_file('ztrue_data', TableHandle, ztrue_file)\n",
    "ztrue = ztrue_data()['photometry']['redshift']\n",
    "zgrid = fzdata().metadata()['xvals'].ravel()\n",
    "photoz_mode = fzdata().mode(grid=zgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qp.metrics.pit import PIT\n",
    "pitobj = PIT(fzdata(), ztrue)\n",
    "quant_ens = pitobj.pit\n",
    "metamets = pitobj.calculate_pit_meta_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate method PIT class returns two objects, a quantile distribution based on the full set of PIT values (a frozen distribution object), and a dictionary of meta metrics associated to PIT (to be detailed below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metamets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_vals = np.array(pitobj.pit_samps)\n",
    "pit_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_out_rate = metamets['outlier_rate']\n",
    "print(f\"PIT outlier rate of this sample: {pit_out_rate:.6f}\") \n",
    "pit_out_rate = pitobj.evaluate_PIT_outlier_rate()\n",
    "print(f\"PIT outlier rate of this sample: {pit_out_rate:.6f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIT-QQ plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of PIT values is a useful tool for a qualitative assessment of PDFs quality. It shows whether the PDFs are:\n",
    "* biased (tilted PIT histogram)\n",
    "* under-dispersed (excess counts close to the boudaries 0 and 1)\n",
    "* over-dispersed (lack of counts close the boudaries 0 and 1)\n",
    "* well-calibrated (flat histogram)\n",
    "\n",
    "Following the standards in DC1 paper, the PIT histogram is accompanied by the quantile-quantile (QQ), which can be used to compare qualitatively the PIT distribution obtained with the PDFs agaist the ideal case (uniform distribution). The closer the QQ plot is to the diagonal, the better is the PDFs calibration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_pit_qq, ks_plot\n",
    "pdfs = fzdata.data.objdata()['yvals']\n",
    "plot_pit_qq(pdfs, zgrid, ztrue, title=\"PIT-QQ - toy data\", code=\"FlexZBoost\",\n",
    "                pit_out_rate=pit_out_rate, savefig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black horizontal line represents the ideal case where the PIT histogram would behave as a uniform distribution U(0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics of CDF-based metrics\n",
    "\n",
    "To evaluate globally the quality of PDFs estimates, `rail.evaluation` provides a set of metrics to compare the empirical distributions of PIT values with the reference uniform distribution, U(0,1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov  \n",
    "\n",
    "Let's start with the traditional Kolmogorov-Smirnov (KS) statistic test, which is the maximum difference between the empirical and the expected cumulative distributions of PIT values:\n",
    "\n",
    "$$\n",
    "\\mathrm{KS} \\equiv \\max_{PIT} \\Big( \\left| \\ \\mathrm{CDF} \\small[ \\hat{f}, z \\small] - \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\  \\right| \\Big)\n",
    "$$\n",
    "\n",
    "Where $\\hat{f}$ is the PIT distribution and $\\tilde{f}$ is U(0,1). Therefore, the smaller value of KS the closer the PIT distribution is to be uniform. The `evaluate` method of the PITKS class returns a named tuple with the statistic and p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_stat_and_pval = metamets['ks']\n",
    "print(f\"PIT KS stat and pval: {ks_stat_and_pval}\") \n",
    "ks_stat_and_pval = pitobj.evaluate_PIT_KS()\n",
    "print(f\"PIT KS stat and pval: {ks_stat_and_pval}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_plot(pitobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"KS metric of this sample: {ks_stat_and_pval.statistic:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramer-von Mises\n",
    "\n",
    "Similarly, let's calculate the Cramer-von Mises (CvM) test, a variant of the KS statistic defined as the mean-square difference between the CDFs of an empirical PDF and the true PDFs:\n",
    "\n",
    "$$ \\mathrm{CvM}^2 \\equiv \\int_{-\\infty}^{\\infty} \\Big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\Big)^{2} \\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n",
    "on the distribution of PIT values, which should be uniform if the PDFs are perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvm_stat_and_pval = metamets['cvm']\n",
    "print(f\"PIT CvM stat and pval: {cvm_stat_and_pval}\") \n",
    "cvm_stat_and_pval = pitobj.evaluate_PIT_CvM()\n",
    "print(f\"PIT CvM stat and pval: {cvm_stat_and_pval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CvM metric of this sample: {cvm_stat_and_pval.statistic:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling \n",
    "\n",
    "Another variation of the KS statistic is the Anderson-Darling (AD) test, a weighted mean-squared difference featuring enhanced sensitivity to discrepancies in the tails of the distribution. \n",
    "\n",
    "$$ \\mathrm{AD}^2 \\equiv N_{tot} \\int_{-\\infty}^{\\infty} \\frac{\\big( \\mathrm{CDF} \\small[ \\hat{f}, z \\small] \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)^{2}}{\\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big( 1 \\ - \\ \\mathrm{CDF} \\small[ \\tilde{f}, z \\small] \\big)}\\mathrm{dCDF}(\\tilde{f}, z) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_stat_crit_sig = metamets['ad']\n",
    "print(f\"PIT AD stat and pval: {ad_stat_crit_sig}\") \n",
    "ad_stat_crit_sig = pitobj.evaluate_PIT_anderson_ksamp()\n",
    "print(f\"PIT AD stat and pval: {ad_stat_crit_sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AD metric of this sample: {ad_stat_crit_sig.statistic:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to remove catastrophic outliers before calculating the integral for the sake of preserving numerical instability. For instance, Schmidt et al. computed the Anderson-Darling statistic within the interval (0.01, 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_stat_crit_sig_cut = pitobj.evaluate_PIT_anderson_ksamp(pit_min=0.01, pit_max=0.99)\n",
    "print(f\"AD metric of this sample: {ad_stat_crit_sig.statistic:.4f}\") \n",
    "print(f\"AD metric for 0.01 < PIT < 0.99: {ad_stat_crit_sig_cut.statistic:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDE Loss\n",
    "\n",
    "In the absence of true photo-z posteriors, the metric used to evaluate individual PDFs is the **Conditional Density Estimate (CDE) Loss**, a metric analogue to the root-mean-squared-error:\n",
    "\n",
    "$$ L(f, \\hat{f}) \\equiv  \\int \\int {\\big(f(z | x) - \\hat{f}(z | x) \\big)}^{2} dzdP(x), $$ \n",
    "\n",
    "where $f(z | x)$ is the true photo-z PDF and $\\hat{f}(z | x)$ is the estimated PDF in terms of the photometry $x$. Since $f(z | x)$  is unknown, we estimate the **CDE Loss** as described in [Izbicki & Lee, 2017 (arXiv:1704.08095)](https://arxiv.org/abs/1704.08095). :\n",
    "\n",
    "$$ \\mathrm{CDE} = \\mathbb{E}\\big(  \\int{{\\hat{f}(z | X)}^2 dz} \\big) - 2{\\mathbb{E}}_{X, Z}\\big(\\hat{f}(Z, X) \\big) + K_{f},  $$\n",
    "\n",
    "\n",
    "where the first term is the expectation value of photo-z posterior with respect to the marginal distribution of the covariates X, and the second term is the expectation value  with respect to the joint distribution of observables X and the space Z of all possible redshifts (in practice, the centroids of the PDF bins), and the third term is a constant depending on the true conditional densities $f(z | x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.evaluation.metrics.cdeloss import *\n",
    "cdelossobj = CDELoss(fzdata.data, zgrid, ztrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_stat_and_pval = cdelossobj.evaluate()\n",
    "cde_stat_and_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CDE loss of this sample: {cde_stat_and_pval.statistic:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
