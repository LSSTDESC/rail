{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b20ea02",
   "metadata": {},
   "source": [
    "# Using Photometry to Estimate Photometric Redshifts\n",
    "\n",
    "**Authors:** Jennifer Scora, Tai Withers, Mubdi Rahman\n",
    "\n",
    "**Last run successfully:** Feb 9, 2026\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35aaca",
   "metadata": {},
   "source": [
    "This notebook covers the basics of using real galaxy photometry to estimate photometric redshifts with RAIL. We will use a couple of the RAIL algorithms to do this, to get a sense of the differences between algorithms and how they work. We'll go through the following steps:\n",
    "\n",
    "1. Setting up your data\n",
    "2. Estimating redshifts with `k-Nearest Neighbours` (KNN)\n",
    "    * this includes how to save the redshifts to file or convert them to other data formats \n",
    "3. Estimating redshifts with `FlexZBoost`\n",
    "\n",
    "Before we get started, here's a quick introduction to some of the features of RAIL interactive mode. The only RAIL package you need to import is the `rail.interactive` package. This contains all of the interactive functions for all of the RAIL algorithms. You may need to import supporting functions that are not part of a stage separately. To get a sense of what functions/stages are available and for some more detailed instructions, see [the RAIL documentation](https://rail-hub.readthedocs.io/en/latest/source/user_guide/interactive_usage.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import qp\n",
    "\n",
    "# import the packages we'll need\n",
    "import rail.interactive as ri\n",
    "import tables_io\n",
    "from rail.utils.path_utils import find_rail_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f94756",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using estimation algorithms, which can all be found under the `ri.estimation.algos` namespace. You can see a list of [existing algorithms](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html), or you can also explore the available options using tab-complete. Each algorithm will have its own namespace, for example, the namespace for KNN is `k_nearneigh`. Each of these algorithms will then have an `informer` and `estimator` method.\n",
    "\n",
    "To get the docstrings for a function, including what parameters it needs and what it returns, you can just put a question mark after the function call or use the `help()` function, as you would with any python function. For example, we'll be using the KNN estimator function later, so we can take a look at what it needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c192655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.estimation.algos.k_nearneigh.k_near_neigh_estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505e697",
   "metadata": {},
   "source": [
    "## 1. Setting up your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57156c",
   "metadata": {},
   "source": [
    "In this notebook we'll be using the ['Estimation' stage](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html) of RAIL. The estimation algorithms, or `Estimators`, have both an *inform* method and an *estimation* method:\n",
    "\n",
    "- **Inform**: calibrates the photometric redshift estimation algorithms, so it requires both photometry data and the true redshifts of the galaxies. \n",
    "- **Estimation:** uses the calibrated algorithm to estimate the redshifts of the galaxies you're interested in given their photometry. \n",
    "\n",
    "This means we'll need two separate data sets, one for calibration and one for estimating on. We'll start by getting those data sets set up. `test_dc2_training_9816.hdf5` is what we'll use for calibrating the *inform* method, and `test_dc2_validation_9816.hdf5` will act as our 'real' galaxy photometry, or target data set, which we will provide to the *estimation* method to get our photometric redshifts (photo-z). \n",
    "\n",
    "Both files contain data drawn from the cosmoDC2_v1.1.4 truth extragalactic catalog generated by DESC with model 10-year-depth magnitude uncertainties.  The calibration data contains roughly 10,000 galaxies, while the target data contains roughly 20,000. In a real-world scenario, you'll be bringing your own \"test\" data at least, but also likely your calibration data set as well. \n",
    "\n",
    "First, we'll use the `find_rail_file` function to get the full path to the data files mentioned above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67822fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data_file = find_rail_file(\n",
    "    \"examples_data/testdata/test_dc2_training_9816.hdf5\"\n",
    ")\n",
    "test_data_file = find_rail_file(\"examples_data/testdata/test_dc2_validation_9816.hdf5\")\n",
    "print(calibration_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2d76f",
   "metadata": {},
   "source": [
    "Now let's read these files in. We'll start by reading in the calibration data, and converting it to a Pandas DataFrame to make it easier to read.\n",
    "\n",
    "We use the [tables_io](https://tables-io.readthedocs.io) package in order to read in these HDF5 files.\n",
    "\n",
    "**NOTE:** Your data doesn't have to be in HDF5 files. The only main requirement is that the data in-memory is in a table format, such as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "calibration_data = tables_io.read(calibration_data_file)\n",
    "print(type(calibration_data), calibration_data.keys())\n",
    "\n",
    "# get the data table out of the photometry dictionary and convert to pandas DataFrame\n",
    "calibration_data = calibration_data[\"photometry\"]\n",
    "calibration_data = tables_io.convert(calibration_data, \"pandasDataFrame\")\n",
    "print(calibration_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3813e",
   "metadata": {},
   "source": [
    "The `calibration_data` is now a Pandas DataFrame, containing information on 10,225 galaxies. It has magnitudes for the *ugrizy* bands, including errors, and the true redshift of these galaxies. \n",
    "\n",
    "Take a note of the column names -- these are the default expected column names for most of the RAIL estimation stages. Of course, your actual calibration or target data sets are not likely to have these exact column names. In that case, there are a few optional parameters that need to be changed when running both the *inform* and *estimation* algorithms:\n",
    "- `hdf5_groupname`: This is the key used to access your data table from a dictionary, for example if it was stored in an HDF5 file. If you are passing a data table directly, just give it `\"\"`\n",
    "- `bands`: This is a list of the column names with your photometry bands, i.e. `['u','g','r','i','z','y']`\n",
    "- `err_bands`: This is a list of the error column names that are associated with the photometry bands, i.e. `['u_err','g_err'...]`\n",
    "- `mag_limits`: This is a dictionary of magnitude limits for each band, i.e. `['u': 28.0, 'g': 29.1 ...]`\n",
    "- `ref_band`: This is the column name of a specific band to use in addition to colors, i.e. `'i'`\n",
    "\n",
    "For an example of this you can check out [16_Running_with_different_data.ipynb](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/estimation_examples/16_Running_with_different_data.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c0e3b",
   "metadata": {},
   "source": [
    "We'll now also load in the target data, which contains the magnitudes for the galaxies we actually want to calculate redshifts for. Just as an example, we'll leave the target data in the default format given by `tables_io` for an `hdf5` file, which is a dictionary of arrays. Either method can be used with RAIL functions, but they can require slightly different methods of passing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tables_io.read(test_data_file)\n",
    "print(test_data[\"photometry\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1178e00",
   "metadata": {},
   "source": [
    "## 2. Estimate redshifts with the [KNN algorithm](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html#k-nearest-neighbor) \n",
    "\n",
    "**The algorithm**:  The `k-Nearest Neighbours` algorithm we're using (see [here](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) for more of an explanation of how KNN works) is a wrapper around `sklearn`'s nearest neighbour (NN) machine learning model. Essentially, it takes a given galaxy, identifies its nearest neighbours in the space, in this case galaxies that have similar colours, and then constructs the photometric redshift PDF as a sum of Gaussians from each neighbour.\n",
    "\n",
    "**Inform**: The inform method is calibrating the model that we will use to estimate the redshifts. It will set aside some of the calibration data set as a validation data set. We will plug in our calibration data set, and any parameters the model needs, which we can check by putting a question mark after the function name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.estimation.algos.k_nearneigh.k_near_neigh_informer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a2d51",
   "metadata": {},
   "source": [
    "There are a lot of optional parameters. Some of the main ones to be aware of for the KNN algorithm are:\n",
    "- `trainfrac` sets the proportion of calibration data to use in training the algorithm, where the remaining fraction is used to validate both the width of the Gaussians used in constructing the PDF and the number of neighbors used in each PDF.  \n",
    "- `sigma_grid_min`, `sigma_grid_max`, and `ngrid_sigma` are used to specify the grid of sigma values to test for the Gaussians \n",
    "- `nneigh_min` and `nneigh_max` set the range of nearest neighbours that will be tested \n",
    "- `zmin`, `zmax`, and `nzbins` are used to create a grid of redshift points on which to validate the model \n",
    "\n",
    "The only required parameter is the calibration data (called `training_data`). We'll also need to include `hdf5_groupname = \"\"`, which just tells the code that there is no dictionary key it needs to use to get to the data, since we're just passing it a DataFrame directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to pass to the informer\n",
    "knn_dict = dict(\n",
    "    zmin=0.0,\n",
    "    zmax=3.0,\n",
    "    nzbins=301,\n",
    "    trainfrac=0.75,\n",
    "    sigma_grid_min=0.01,\n",
    "    sigma_grid_max=0.07,\n",
    "    ngrid_sigma=10,\n",
    "    nneigh_min=3,\n",
    "    nneigh_max=7,\n",
    "    hdf5_groupname=\"\",\n",
    ")\n",
    "\n",
    "# run the inform method\n",
    "knn_inform = ri.estimation.algos.k_nearneigh.k_near_neigh_informer(\n",
    "    training_data=calibration_data, **knn_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99485baf",
   "metadata": {},
   "source": [
    "Now, if you take a look at the output of this function, you can see that it's a dictionary with the key 'model', since that's what we're generating, and the actual model object as the value. If there were multiple outputs for this function, they would all be collected in this dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_inform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0ec7a",
   "metadata": {},
   "source": [
    "To assess how well this model has been calibrated, you can take a look at the [algorithm documentation] for more information about it, and you can explore the [introduction to RAIL interactive](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/estimation_examples/Estimating_Redshifts_and_Comparing_Results_for_Different_Parameters.html) or the [01_Evaluation_by_Type](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/evaluation_examples/01_Evaluation_by_Type.html) notebooks, which go into more detail about the process of evaluating the performance of estimation algorithm models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdff9",
   "metadata": {},
   "source": [
    "### Saving a model and using it with an estimator stage\n",
    "\n",
    "The output of the inform stage is just a dictionary with the model under the \"model\" key. To make our lives easier, we can save this model to a file. That way, we only have to run the *estimator* method in the future, and supply the file name of the model we've just saved. This will make it so you can use this calibration without running the *informer* stage again. \n",
    "\n",
    "Let's start by saving the file (we recommend using the `pickle` module since that's what format most estimation algorithms expect the model to be in):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model file out here\n",
    "with open(\"./knn_model.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(obj=knn_inform[\"model\"], file=fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c055f73",
   "metadata": {},
   "source": [
    "### Estimate \n",
    "\n",
    "Now that our model is calibrated, we can use it to estimate the redshifts of the target data set. We provide the estimate algorithm with the target data set, and the filename of the model that we've trained, and any other necessary parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_estimated = ri.estimation.algos.k_nearneigh.k_near_neigh_estimator(\n",
    "    input_data=test_data, model=\"knn_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d12cc0",
   "metadata": {},
   "source": [
    "### Taking a look at your estimated redshifts \n",
    "\n",
    "Typically for photometric redshifts you would expect the algorithm to give you just a number with error bars. RAIL provides more detailed information about the photometric redshift estimate, so we give you the full probability distribution function for each target galaxy. This is a little more complex than just a table of numbers, so we use the `qp` library and its `Ensemble` data structure to help store it. \n",
    "\n",
    "Now let's take a look at what the output of the estimation stage actually looks like. Most estimation stages output an `Ensemble`, which is a data structure from the package `qp`. For more information, see [the qp documentation](https://qp.readthedocs.io/en/main/user_guide/datastructure.html). \n",
    "\n",
    "We're using an `Ensemble` to hold a redshift distribution for each of the galaxies we're estimating. There are two required dictionaries that make up an Ensemble, and one that is optional:\n",
    "- `.metadata`: Contains information about the whole data structure, like the Ensemble type, and any shared parameters such as the bins of histograms. This is not per-object metadata. \n",
    "- `.objdata`: The main data points of the distributions for each object, where each object is a row. \n",
    "- `.ancil`: the optional dictionary, containing extra information about each object. It can have arrays that have one or more data points per distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92480622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c909c",
   "metadata": {},
   "source": [
    "We can see that this algorithm outputs Ensembles of class `mixmod`, which are just combinations of Gaussians (for more info see the [qp docs](https://qp.readthedocs.io/en/main/user_guide/parameterizations/mixmod.html)). The shape portion of the print statement tells us two things: the first number is the number of photo-z distributions, or galaxies, in this `Ensemble`, and the second number tells us how many Gaussians are combined to make up each photo-z distribution. \n",
    "\n",
    "Let's take a look at what the different dictionaries look like for this `Ensemble`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated[\"output\"].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated[\"output\"].objdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12036268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated[\"output\"].ancil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c3203",
   "metadata": {},
   "source": [
    "The KNN algorithm automatically adds a photo-z point estimate derived from the PDFs, called 'zmode' in the ancillary dictionary above, but this is not the case for all Estimation algorithms, and we recommend choosing your own point estimate based on what you want to use it for. \n",
    "\n",
    "The `Ensemble` acts a bit like a [`scipy` probability distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.html#scipy.stats.rv_continuous), so you can easily calculate statistics from it, such as the mean, median, or mode of all of the photo-z PDFs. For a full list of the functions available see [the list of methods here](https://qp.readthedocs.io/en/main/user_guide/methods.html).\n",
    "\n",
    "Another extremely useful method is the `.pdf()` method, which calculates the value(s) of the distribution(s) at any redshift(s) you provide. This is quite useful for plotting, since the different estimation algorithms can store data in different ways, which makes plotting the actual data points more confusing, but we can always use the `.pdf()` method. \n",
    "\n",
    "We'll make use of this method to plot a couple of the photo-z distributions that the algorithm has generated to get a sense of what they're like. In order to do this, we'll need to select a specific galaxy to get the PDF for, which can be done by slicing as you would with a numpy array. So to get the first galaxy's distribution, for example you would do this: `knn_estimated[\"output\"][0]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "xvals = np.linspace(0, 3, 200)  # we want to cover the whole available redshift space\n",
    "plt.plot(xvals, knn_estimated[\"output\"][0].pdf(xvals), label=\"0\")\n",
    "plt.plot(xvals, knn_estimated[\"output\"][1000].pdf(xvals), label=\"1000\")\n",
    "plt.plot(xvals, knn_estimated[\"output\"][10000].pdf(xvals), label=\"10,000\")\n",
    "\n",
    "plt.legend(loc=\"best\", title=\"Galaxy ID\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6df77",
   "metadata": {},
   "source": [
    "We can see that these distributions have varying shapes. Importantly, you can see most of them don't just look like one Guassian, but have multiple Gaussian peaks. This explains why you shouldn't just use a point estimate with some error bars, since that doesn't accurately encompass the probability distribution. \n",
    "\n",
    "However, if you do want a point estimate, you should think about what you want the point estimate to represent. It's easy enough to calculate either the mean, median, or mode with the built-in functions, which means you can pick which one works best for your goals. For this example, we'll go with the median value, which we calculate via: `knn_estimated[\"output\"][galid].median()`, and we'll compare it to the actual redshift for that galaxy to get a sense of how they compare: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "zgrid = np.linspace(0, 3.0, 301)\n",
    "galid = 1000\n",
    "truez = test_data[\"photometry\"][\"redshift\"][galid]\n",
    "single_gal = np.squeeze(knn_estimated[\"output\"][galid].pdf(zgrid))\n",
    "single_zmed = knn_estimated[\"output\"][galid].median()\n",
    "\n",
    "plt.plot(zgrid, single_gal, color=\"k\", label=\"single pdf\")\n",
    "plt.axvline(single_zmed, color=\"k\", ls=\"--\", label=\"mode\")\n",
    "plt.axvline(truez, color=\"r\", label=\"true redshift\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd5f05",
   "metadata": {},
   "source": [
    "In a real world scenario you won't be able to make this plot, but we do it here just to illustrate how the estimators work. We can see there is some difference between the true redshift and the estimated redshift point estimate, though for this galaxy the point estimate seems quite good. Let's see how the algorithm did overall by plotting the pre-calculated redshift point estimates (the \"zmodes\") versus the true redshifts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2abfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    test_data[\"photometry\"][\"redshift\"],\n",
    "    knn_estimated[\"output\"].ancil[\"zmode\"].flatten(),\n",
    "    s=1,\n",
    "    c=\"k\",\n",
    "    label=\"simple NN mode\",\n",
    ")\n",
    "plt.plot([0, 3], [0, 3], \"r--\")\n",
    "plt.xlabel(\"true redshift\")\n",
    "plt.ylabel(\"estimated photo-z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88268f52",
   "metadata": {},
   "source": [
    "We can see that the algorithm does quite well overall, though there are certainly some catastrophic outliers, and more of a spread at higher redshifts. \n",
    "\n",
    "### Saving the estimated redshift distributions to a file\n",
    "\n",
    "Now that we've investigated our redshifts, let's say that we want to save them to a file for later. `Ensembles` come with a built in `write_to()` function that allows you to write it to an HDF5, FITS, or parquet file. Just provide the function with the path to the file you want it to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_estimated[\"output\"].write_to(\"knn_redshift_estimates.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15089502",
   "metadata": {},
   "source": [
    "Now if you want to make use of the `Ensemble` of distributions in the future, you can just read the file in like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2dca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_ens = qp.read(\"knn_redshift_estimates.hdf5\")\n",
    "print(knn_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ab8ec",
   "metadata": {},
   "source": [
    "But what if you want to convert your data to a table, so you can easily input it into other algorithms, for example? One way we might want to do that is by outputting an array where each row gives the values of a photo-z PDF at certain points on a grid of redshifts. We can do this easily by using the `.pdf()` function of an Ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce83bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the points to evaluate the PDFs at \n",
    "zgrid_out = np.linspace(0,3,200)\n",
    "# evaluate all the PDFs at the given redshifts\n",
    "photoz_out = knn_ens.pdf(zgrid_out)\n",
    "photoz_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc286b6",
   "metadata": {},
   "source": [
    "Now we have an array of datapoints that describes all of our photo-z distributions at the set `zgrid_out` redshift values. This array can be saved to a file however you like, or passed on to other functions. \n",
    "\n",
    "You can also use a similar method to create arrays of different data points, for example if you want the CDF values of all the distributions, use `.cdf()`, or if you just want the medians and standard deviations of all the distributions, use `.median()` and `.std()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaee0c",
   "metadata": {},
   "source": [
    "## 3. Estimate redshifts with [FlexZBoost](https://rail-hub.readthedocs.io/en/latest/source/estimators.html#flexzboost)\n",
    "\n",
    "Now let's use `FlexZBoost` to get our redshifts. `FlexZBoostEstimator` approximates the conditional density estimate for each PDF with a set of weights on a set of basis functions.  This can save space relative to a gridded parameterization, but it also leads to residual \"bumps\" in the PDF intrinsic to the underlying cosine or fourier parameterization.  For this reason, `FlexZBoostEstimator` has a post-processing stage where it \"trims\" (i.e. sets to zero) any small peaks, or \"bumps\", below a certain `bump_thresh` threshold. For more details on running `FlexZBoost`, see the [00_Quick_Start_in_Estimation.ipynb](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/estimation_examples/00_Quick_Start_in_Estimation.html) notebook.\n",
    "\n",
    "These are some of the main parameters for the informer:\n",
    " `basis_system`: which basis system to use in the density estimate. The default is `cosine` but `fourier` is also an option\n",
    "- `max_basis`: the maximum number of basis functions parameters to use for PDFs\n",
    "- `regression_params`: a dictionary of options fed to `xgboost` that control the maximum depth and the `objective` function. `objective` should be set to `reg:squarederror` for proper functioning.\n",
    "- `trainfrac`: The fraction of the calibration data to use for training the density estimate.  The remaining galaxies will be used for validation of `bump_thresh` and `sharpening`.\n",
    "- `bumpmin`: the minimum value to test in the `bump_thresh` grid\n",
    "- `bumpmax`: the maximum value to test in the `bump_thresh` grid\n",
    "- `nbump`: how many points to test in the `bump_thresh` grid\n",
    "- `sharpmin`, `sharpmax`, `nsharp`: same as equivalent `bump_thresh` params, but for `sharpening` parameter\n",
    "\n",
    "The dictionary below gives the defaults for all of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_dict = dict(\n",
    "    zmin=0.0,\n",
    "    zmax=3.0,\n",
    "    nzbins=301,\n",
    "    trainfrac=0.75,\n",
    "    bumpmin=0.02,\n",
    "    bumpmax=0.35,\n",
    "    nbump=20,\n",
    "    sharpmin=0.7,\n",
    "    sharpmax=2.1,\n",
    "    nsharp=15,\n",
    "    max_basis=35,\n",
    "    basis_system=\"cosine\",\n",
    "    regression_params={\"max_depth\": 8, \"objective\": \"reg:squarederror\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b5b09",
   "metadata": {},
   "source": [
    "Now we can run the informer with this dictionary, along with the calibration data and the `hdf5_groupname` parameter as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b20312",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_inform = ri.estimation.algos.flexzboost.flex_z_boost_informer(\n",
    "    training_data=calibration_data, hdf5_groupname=\"\", **fz_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064eb93f",
   "metadata": {},
   "source": [
    "Next we run the estimator with the model that we've just created, like the `knn_inform` variable, `flex_inform` is also a dictionary with a \"model\" key.\n",
    "\n",
    "**Note that when we pass the model to this function, we don't pass the dictionary, but the actual model object. This is true of all the interactive functions.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2897f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_estimated = ri.estimation.algos.flexzboost.flex_z_boost_estimator(\n",
    "    input_data=test_data, model=flex_inform[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb88685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flex_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9b35a",
   "metadata": {},
   "source": [
    "We once again get our dictionary with the key \"output\", this time the `Ensemble` is of type [`interp`](https://qp.readthedocs.io/en/main/user_guide/parameterizations/interp.html), which means the distributions are given as a set of `xvals` and `yvals`, where all the distributions share the same set of `xvals`. \n",
    "\n",
    "Now we can plot out some of the data, same as above, to get a sense of how the estimator did. Let's start by plotting some of the individual galaxy photo-z distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(0, 3, 200)  # we want to cover the whole available redshift space\n",
    "plt.plot(xvals, flex_estimated[\"output\"][0].pdf(xvals), label=\"0\")\n",
    "plt.plot(xvals, flex_estimated[\"output\"][1000].pdf(xvals), label=\"1000\")\n",
    "plt.plot(xvals, flex_estimated[\"output\"][10000].pdf(xvals), label=\"10,000\")\n",
    "\n",
    "plt.legend(loc=\"best\", title=\"Galaxy ID\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1066b",
   "metadata": {},
   "source": [
    "We can see that these distributions look a little smoother than the ones we got from the KNN algorithm. Let's compare them by plotting the PDF, median, and true redshift of a galaxy against the KNN-estimated PDF of the same galaxy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate median of photo-z PDF\n",
    "z_med = flex_estimated[\"output\"][1000].median()\n",
    "\n",
    "# get rest of plotting data\n",
    "zgrid = np.linspace(0, 3.0, 301)\n",
    "galid = 1000\n",
    "truez = test_data[\"photometry\"][\"redshift\"][galid]\n",
    "single_gal = np.squeeze(flex_estimated[\"output\"][galid].pdf(zgrid))\n",
    "\n",
    "plt.plot(zgrid, single_gal, color=\"k\", label=\"FlexZBoost\")\n",
    "plt.plot(zgrid, knn_estimated[\"output\"][galid].pdf(zgrid), label=\"KNN\")\n",
    "plt.axvline(z_med, color=\"k\", ls=\"--\", label=\"median\")\n",
    "plt.axvline(truez, color=\"r\", label=\"true redshift\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecb279",
   "metadata": {},
   "source": [
    "As mentioned, the FlexZBoost estimated distribution for this galaxy is a lot cleaner, and the point estimate is very close to the true redshift. \n",
    "\n",
    "Finally, we can do this for all the galaxies, and compare the estimated redshifts with the true redshifts as we did for the KNN algorithm earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    test_data[\"photometry\"][\"redshift\"],\n",
    "    flex_estimated[\"output\"].median(),\n",
    "    s=1,\n",
    "    c=\"k\",\n",
    "    label=\"FlexZBoost median\",\n",
    ")\n",
    "plt.plot([0, 3], [0, 3], \"r--\")\n",
    "plt.xlabel(\"true redshift\")\n",
    "plt.ylabel(\"estimated photo-z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab838",
   "metadata": {},
   "source": [
    "This distribution actually looks quite similar to our distribution of KNN estimated values -- in both cases, they're quite good with some outliers at higher redshift. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cac91",
   "metadata": {},
   "source": [
    "## 4. Next steps\n",
    "\n",
    "Now that you've got a handle on estimating photometric redshifts with these algorithms, take a look at the other [available estimation algorithms](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html#) that you can use. If you want some more detail and some examples of how to use them, you can explore the [estimation notebooks](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/estimation_notebooks.html). \n",
    "\n",
    "If you're interested in parallelizing the estimation method within a notebook, or in learning about any of the other parts of RAIL, try [this notebook](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/estimation_examples/Estimating_Redshifts_and_Comparing_Results_for_Different_Parameters.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
