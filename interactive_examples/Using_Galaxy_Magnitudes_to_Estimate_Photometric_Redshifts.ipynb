{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b20ea02",
   "metadata": {},
   "source": [
    "# Using Galaxy Magnitudes to Estimate Photometric Redshifts\n",
    "\n",
    "This notebook covers the basics of using real galaxy magnitudes to estimate photometric redshifts with RAIL. We will use a couple of the RAIL algorithms to do this, to get a sense of the differences between algorithms and how they work. We'll go through the following steps:\n",
    "\n",
    "1. Setting up training and testing data sets \n",
    "2. Estimating redshifts with `k-Nearest Neighbours` (KNN)\n",
    "3. Estimating redshifts with `FlexZBoost`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35aaca",
   "metadata": {},
   "source": [
    "Before we get started, here's a quick introduction to some of the features of RAIL interactive mode. The only RAIL package you need to import is the `rail.interactive` package. This contains all of the interactive functions for all of the RAIL algorithms. You may need to import supporting functions that are not part of a stage separately. To get a sense of what functions/stages are available and for some more detailed instructions, see [the RAIL documentation](https://descraildocs.z27.web.core.windows.net/source/user_guide/interactive_usage.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages we'll need\n",
    "import rail.interactive as ri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f94756",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using estimation algorithms, which can all be found under the `ri.estimation.algos` namespace. You can see a list of existing algorithms (here), or you can also explore the available options using tab-complete. Each algorithm will have its own namespace, for example, the namespace for KNN is `k_nearneigh`. Each of these algorithms will then have an `informer` and `estimator` method.\n",
    "\n",
    "To get the docstrings for a function, including what parameters it needs and what it returns, you can just put a question mark after the function call or use the `help()` function, as you would with any python function. For example, we'll be using the KNN estimator function later, so we can take a look at what it needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c192655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.estimation.algos.k_nearneigh.k_near_neigh_estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505e697",
   "metadata": {},
   "source": [
    "## 1. Setting up training and testing data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57156c",
   "metadata": {},
   "source": [
    "In this notebook we'll be using the ['Estimation' stage](https://descraildocs.z27.web.core.windows.net/source/rail_stages/estimation.html) of RAIL. The estimation algorithms, or `Estimators`, have both an *inform* method and an *estimation* method. The inform method trains the model that will be used to estimate the redshifts, so that will need to be given both magnitude data and the true redshifts of the galaxies. We can then pass a new set of magnitudes (the ones we're actually interested in) to the *estimator*, along with the model that the informer created. The estimator can then apply the model to the new magnitudes in order to calculate a redshift value. \n",
    "\n",
    "This means we'll need two separate data sets for each of the methods. We'll start by getting those data sets set up. `test_dc2_training_9816.hdf5` is what we'll use for training the *inform* method, and `test_dc2_validation_9816.hdf5` will act as our 'real' galaxy magnitude data, which we will provide to the *estimation* method to get our photometric redshifts (photo-z). Both files contain data drawn from the cosmoDC2_v1.1.4 truth extragalactic catalog generated by DESC with model 10-year-depth magnitude uncertainties.  The training data contains roughly 10,000 galaxies, while the test data contains roughly 20,000.  Both sets are representative down to a limiting apparent magnitude. \n",
    "\n",
    "First, we'll use the `find_rail_file` function to get the full path to the data files mentioned above. If you have your own data, you can substitute in the path to that file for the value of `testFile` below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67822fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tables_io\n",
    "from rail.utils.path_utils import find_rail_file\n",
    "\n",
    "trainFile = find_rail_file(\"examples_data/testdata/test_dc2_training_9816.hdf5\")\n",
    "testFile = find_rail_file(\"examples_data/testdata/test_dc2_validation_9816.hdf5\")\n",
    "print(trainFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2d76f",
   "metadata": {},
   "source": [
    "Now let's read these files in. We'll start by reading in the training data, and converting it to a Pandas dataframe to make it easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57842b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "training_data = tables_io.read(trainFile)\n",
    "print(type(training_data), training_data.keys())\n",
    "\n",
    "# get the data table out of the photometry dictionary and convert to pandas DataFrame\n",
    "training_data = training_data[\"photometry\"]\n",
    "training_data = tables_io.convert(training_data, \"pandasDataFrame\")\n",
    "print(training_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c0e3b",
   "metadata": {},
   "source": [
    "`training_data` is now a Pandas DataFrame, containing information on 10,225 galaxies. It has magnitude information for the *ugrizy* bands, including errors, and the true redshift of these galaxies.\n",
    "\n",
    "We'll now also load in the test data, which contains the magnitudes for the galaxies we actually want to calculate redshifts for. Just as an example, we'll leave the test data in the default format given by `tables_io` for an `hdf5` file, which is a dictionary of arrays. Either method can be used with RAIL functions, but they can require slightly different methods of passing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tables_io.read(testFile)\n",
    "print(test_data[\"photometry\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1178e00",
   "metadata": {},
   "source": [
    "## 2. Estimate redshifts with the [KNN algorithm](https://rail-hub.readthedocs.io/en/latest/source/estimators.html#k-nearest-neighbor) \n",
    "\n",
    "**The algorithm**:  The `k-Nearest Neighbours` algorithm we're using (see [here](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) for more of an explanation of how KNN works) is a wrapper around `sklearn`'s nearest neighbour (NN) machine learning model. Essentially, it takes a given galaxy, identifies its nearest neighbours in the space, in this case galaxies that have similar colours, and then constructs the photometric redshift PDF as a sum of Gaussians from each neighbour.\n",
    "\n",
    "**Inform**: The inform method is training the model that we will use to estimate the redshifts. It will set aside some of the training data set as a validation data set. We will plug in our training data set, and any parameters the model needs, which we can check by putting a question mark after the function name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7163283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.estimation.algos.k_nearneigh.k_near_neigh_informer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a2d51",
   "metadata": {},
   "source": [
    "There are a lot of optional parameters. Some of the main ones to be aware of for the KNN algorithm are:\n",
    "- `trainfrac` sets the proportion of training data to use in training the algorithm, where the remaining fraction is used to validate both the width of the Gaussians used in constructing the PDF and the number of neighbors used in each PDF.  \n",
    "- `sigma_grid_min`, `sigma_grid_max`, and `ngrid_sigma` are used to specify the grid of sigma values to test for the Gaussians \n",
    "- `nneigh_min` and `nneigh_max` set the range of nearest neighbours that will be tested \n",
    "- `zmin`, `zmax`, and `nzbins` are used to create a grid of redshift points on which to validate the model \n",
    "\n",
    "The only required parameter is the training data (called `input`). We'll also need to include `hdf5_groupname = \"\"`, since we've pulled the data table out of the `photometry` group when we converted to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_dict = dict(\n",
    "    zmin=0.0,\n",
    "    zmax=3.0,\n",
    "    nzbins=301,\n",
    "    trainfrac=0.75,\n",
    "    sigma_grid_min=0.01,\n",
    "    sigma_grid_max=0.07,\n",
    "    ngrid_sigma=10,\n",
    "    nneigh_min=3,\n",
    "    nneigh_max=7,\n",
    "    hdf5_groupname=\"\",\n",
    ")\n",
    "knn_inform = ri.estimation.algos.k_nearneigh.k_near_neigh_informer(\n",
    "    input=training_data, **knn_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99485baf",
   "metadata": {},
   "source": [
    "Now, if you take a look at the output of this function, you can see that it's a dictionary with the key 'model', since that's what we're generating, and the actual model object as the value. If there were multiple outputs for this function, they would all be collected in this dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_inform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdff9",
   "metadata": {},
   "source": [
    "### Saving a model and using it with an estimator stage\n",
    "We can see that the output of the inform stage is a dictionary with the model under the \"model\" key. To make our lives easier, we can save this model to a file. That way, we only have to run the `estimator` method in the future, and supply the file name of the model we've just saved. This will speed up our data analysis. Let's start by saving the file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write model file out here\n",
    "with open(\"./knn_model.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(obj=knn_inform[\"model\"], file=fout, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c055f73",
   "metadata": {},
   "source": [
    "**Estimate**: Now that our model is trained, we can use it to estimate the redshifts of the test data set. We provide the estimate algorithm with the test data set, and the filename of the model that we've trained, and any other necessary parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_estimated = ri.estimation.algos.k_nearneigh.k_near_neigh_estimator(\n",
    "    input=test_data, model=\"knn_model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d12cc0",
   "metadata": {},
   "source": [
    "Now let's take a look at what the output of the estimation stage actually looks like. Most estimation stages output an `Ensemble`, which is a data structure from the package `qp`. For more information, see [the qp documentation](https://qp.readthedocs.io/en/main/user_guide/datastructure.html). \n",
    "\n",
    "We're using an `Ensemble` to hold a redshift distribution for each of the galaxies we're estimating. There are two required dictionaries that make up an Ensemble, and one that is optional:\n",
    "- `.metadata`: Contains information about the whole data structure, like the Ensemble type, and any shared parameters such as the bins of histograms. This is not per-object metadata. \n",
    "- `.objdata`: The main data points of the distributions for each object, where each object is a row. \n",
    "- `.ancil`: the optional dictionary, containing extra information about each object. It can have arrays that have one or more data points per distribution. Typically the ancillary data table includes a photo-z point estimate derived from the PDFs, the mode by default, called 'zmode' in the ancillary dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92480622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c909c",
   "metadata": {},
   "source": [
    "We can see that this algorithm outputs Ensembles of class `mixmod`, which are just combinations of Gaussians (for more info see the [qp docs](https://qp.readthedocs.io/en/main/user_guide/parameterizations/mixmod.html)). The shape portion of the print statement tells us two things: the first number is the number of photo-z distributions, or galaxies, in this `Ensemble`, and the second number tells us how many Gaussians are combined to make up each photo-z distribution. \n",
    "\n",
    "Let's take a look at what the different dictionaries look like for this `Ensemble`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b13beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated[\"output\"].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35c3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated[\"output\"].objdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12036268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_estimated[\"output\"].ancil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b3865",
   "metadata": {},
   "source": [
    "We can also get a slice of distributions or a single specific distribution by slicing the Ensemble, for example: `knn_estimated[\"output\"][0]` would give us the first distribution. \n",
    "\n",
    "Now we can take a look at what these photo-z distributions actually look like by plotting them. You can use the `.plot_native` method to do this quickly, but if you want to make your own plots you can use the `.pdf` function which takes an array of redshift values and returns the photo-z probability distribution function at those values, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "xvals = np.linspace(0, 3, 200)  # we want to cover the whole available redshift space\n",
    "plt.plot(xvals, knn_estimated[\"output\"][0].pdf(xvals), label=\"0\")\n",
    "plt.plot(xvals, knn_estimated[\"output\"][1000].pdf(xvals), label=\"1000\")\n",
    "plt.plot(xvals, knn_estimated[\"output\"][10000].pdf(xvals), label=\"10 000\")\n",
    "\n",
    "plt.legend(loc=\"best\", title=\"Galaxy ID\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6df77",
   "metadata": {},
   "source": [
    "We can see that these distributions have varying shapes. To get a point estimate of the redshift, we can use the `zmode` value from the ancillary dictionary. Also, for this notebook our test data has real redshifts to compare against. Let's take the 1000th galaxy's distribution and compare it to these two values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "zgrid = np.linspace(0, 3.0, 301)\n",
    "galid = 1000\n",
    "truez = test_data[\"photometry\"][\"redshift\"][galid]\n",
    "single_gal = np.squeeze(knn_estimated[\"output\"][galid].pdf(zgrid))\n",
    "single_zmode = knn_estimated[\"output\"].ancil[\"zmode\"][galid]\n",
    "\n",
    "plt.plot(zgrid, single_gal, color=\"k\", label=\"single pdf\")\n",
    "plt.axvline(single_zmode, color=\"k\", ls=\"--\", label=\"mode\")\n",
    "plt.axvline(truez, color=\"r\", label=\"true redshift\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd5f05",
   "metadata": {},
   "source": [
    "We can see there is some difference between the true redshift and the estimated redshift point estimate, though for this galaxy the point estimate seems quite good. Let's see how the algorithm did overall by plotting the redshift point estimates (the \"zmodes\") versus the true redshifts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2abfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    test_data[\"photometry\"][\"redshift\"],\n",
    "    knn_estimated[\"output\"].ancil[\"zmode\"].flatten(),\n",
    "    s=1,\n",
    "    c=\"k\",\n",
    "    label=\"simple NN mode\",\n",
    ")\n",
    "plt.plot([0, 3], [0, 3], \"r--\")\n",
    "plt.xlabel(\"true redshift\")\n",
    "plt.ylabel(\"estimated photo-z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88268f52",
   "metadata": {},
   "source": [
    "We can see that the algorithm does quite well overall, though there are certainly some outliers, and more of a spread at higher redshifts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaee0c",
   "metadata": {},
   "source": [
    "## 3. Estimate redshifts with [FlexZBoost](https://rail-hub.readthedocs.io/en/latest/source/estimators.html#flexzboost)\n",
    "\n",
    "Now let's use `FlexZBoost` to get our redshifts. `FlexZBoostEstimator` approximates the conditional density estimate for each PDF with a set of weights on a set of basis functions.  This can save space relative to a gridded parameterization, but it also leads to residual \"bumps\" in the PDF intrinsic to the underlying cosine or fourier parameterization.  For this reason, `FlexZBoostEstimator` has a post-processing stage where it \"trims\" (i.e. sets to zero) any small peaks, or \"bumps\", below a certain `bump_thresh` threshold. For more details on running `FlexZBoost`, see the 'Quick_Start_in_Estimation.ipynb' notebook **TODO: add link**\n",
    "\n",
    "These are some of the main parameters for the informer:\n",
    " `basis_system`: which basis system to use in the density estimate. The default is `cosine` but `fourier` is also an option\n",
    "- `max_basis`: the maximum number of basis functions parameters to use for PDFs\n",
    "- `regression_params`: a dictionary of options fed to `xgboost` that control the maximum depth and the `objective` function. `objective` should be set to `reg:squarederror` for proper functioning.\n",
    "- `trainfrac`: The fraction of the training data to use for training the density estimate.  The remaining galaxies will be used for validation of `bump_thresh` and `sharpening`.\n",
    "- `bumpmin`: the minimum value to test in the `bump_thresh` grid\n",
    "- `bumpmax`: the maximum value to test in the `bump_thresh` grid\n",
    "- `nbump`: how many points to test in the `bump_thresh` grid\n",
    "- `sharpmin`, `sharpmax`, `nsharp`: same as equivalent `bump_thresh` params, but for `sharpening` parameter\n",
    "\n",
    "The dictionary below gives the defaults for all of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fz_dict = dict(\n",
    "    zmin=0.0,\n",
    "    zmax=3.0,\n",
    "    nzbins=301,\n",
    "    trainfrac=0.75,\n",
    "    bumpmin=0.02,\n",
    "    bumpmax=0.35,\n",
    "    nbump=20,\n",
    "    sharpmin=0.7,\n",
    "    sharpmax=2.1,\n",
    "    nsharp=15,\n",
    "    max_basis=35,\n",
    "    basis_system=\"cosine\",\n",
    "    regression_params={\"max_depth\": 8, \"objective\": \"reg:squarederror\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b5b09",
   "metadata": {},
   "source": [
    "Since we're not making any changes to the defaults in this run, we can run the informer with just the input data and the `hdf5_groupname` parameter as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b20312",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_inform = ri.estimation.algos.flexzboost.flex_z_boost_informer(\n",
    "    input=training_data, hdf5_groupname=\"\", **fz_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064eb93f",
   "metadata": {},
   "source": [
    "Next we run the estimator with the model that we've just created, like the `knn_inform` variable, `flex_inform` is also a dictionary with a \"model\" key.\n",
    "\n",
    "**Note that when we pass the model to this function, we don't pass the dictionary, but the actual model object. This is true of all the interactive functions.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2897f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_estimated = ri.estimation.algos.flexzboost.flex_z_boost_estimator(\n",
    "    input=test_data, model=flex_inform[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb88685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flex_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9b35a",
   "metadata": {},
   "source": [
    "We once again get our dictionary with the key \"output\", this time the `Ensemble` is of type `interp`, which means the distributions are given as a set of `xvals` and `yvals`, where all the distributions share the same set of `xvals`. \n",
    "\n",
    "Now we can plot out some of the data, same as above, to get a sense of how the estimator did. Let's start by plotting some of the individual galaxy photo-z distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(0, 3, 200)  # we want to cover the whole available redshift space\n",
    "plt.plot(xvals, flex_estimated[\"output\"][0].pdf(xvals), label=\"0\")\n",
    "plt.plot(xvals, flex_estimated[\"output\"][1000].pdf(xvals), label=\"1000\")\n",
    "plt.plot(xvals, flex_estimated[\"output\"][10000].pdf(xvals), label=\"10000\")\n",
    "\n",
    "plt.legend(loc=\"best\", title=\"Galaxy ID\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1066b",
   "metadata": {},
   "source": [
    "`FlexZBoost` doesn't automatically add \"zmode\" values to the ancillary dictionary of the `Ensemble`, but we can easily calculate redshift point estimates ourselves using `Ensemble` methods `.median()` or `.mode()`. Let's calculate the median for one of the galaxies, and plot it against the PDF and true redshift of galaxy 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate median of photo-z PDF\n",
    "z_med = flex_estimated[\"output\"][1000].median()\n",
    "\n",
    "# get rest of plotting data\n",
    "zgrid = np.linspace(0, 3.0, 301)\n",
    "galid = 1000\n",
    "truez = test_data[\"photometry\"][\"redshift\"][galid]\n",
    "single_gal = np.squeeze(flex_estimated[\"output\"][galid].pdf(zgrid))\n",
    "\n",
    "plt.plot(zgrid, single_gal, color=\"k\", label=\"single pdf\")\n",
    "plt.axvline(z_med, color=\"k\", ls=\"--\", label=\"median\")\n",
    "plt.axvline(truez, color=\"r\", label=\"true redshift\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecb279",
   "metadata": {},
   "source": [
    "Finally, we can do this for all the galaxies, and compare the estimated redshifts with the true redshifts as for the KNN algorithm above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1dbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    test_data[\"photometry\"][\"redshift\"],\n",
    "    flex_estimated[\"output\"].median(),\n",
    "    s=1,\n",
    "    c=\"k\",\n",
    "    label=\"FlexZBoost median\",\n",
    ")\n",
    "plt.plot([0, 3], [0, 3], \"r--\")\n",
    "plt.xlabel(\"true redshift\")\n",
    "plt.ylabel(\"estimated photo-z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab838",
   "metadata": {},
   "source": [
    "This distribution actually looks quite similar to our distribution of KNN estimated values -- in both cases, they're quite good with some outliers at higher redshift. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cac91",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "This step is just to remove the model file we created in Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d679b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.remove(\"knn_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
