{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0f1ec2",
   "metadata": {},
   "source": [
    "# Exploring the Effects of Different Degraders on Estimated Redshifts\n",
    "\n",
    "**Authors:** Jennifer Scora\n",
    "\n",
    "**Last run successfully:** Feb 9, 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f0d18",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore how to create simulated datasets with the [RAIL creation stage](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/creation.html), in particular focusing on how data sets created using different degradation algorithms can affect the training of models to estimate photometric redshifts (photo-zs). Here \"degradation\" algorithms refer to any algorithms applied to alter the \"true\" sample, for example to add biases or cuts. \n",
    "\n",
    "Here are the main steps we'll be following:\n",
    "\n",
    "1. **Create the \"true\" data sets:** use an engine to sample tests and training data sets \n",
    "2. **Create \"degraded\" data sets:** \n",
    "    create multiple training data sets with different degradation algorithms, and one test data set with all the degradation algorithms used \n",
    "3. **Use the data sets to estimate redshifts:** \n",
    "    use the training data sets to train different models, and use each model on the same test data set  \n",
    "4. **Evaluate the estimated redshift distributions and compare them:** \n",
    "    use the evaluator stages to calculate metrics for each model, and assess how certain degraders affect the resulting distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00860981",
   "metadata": {},
   "source": [
    "## 1. Create the \"true\" data sets\n",
    "\n",
    "In this step we'll use the `PZFlow` engine to train a model, and sample from that model to create our \"true\" training and test data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3c980",
   "metadata": {},
   "source": [
    "### Set up\n",
    "\n",
    "Let's start by importing the packages we'll need to create the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rail.interactive as ri\n",
    "import numpy as np\n",
    "from pzflow.examples import get_galaxy_data\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ab4e5",
   "metadata": {},
   "source": [
    "We need to set up some column name dictionaries, as the expected column names vary between some of the codes. In order to handle this, we can pass in  dictionaries of expected column names and the column name that exists in the input data (`band_dict` and `rename_dict` below). In this notebook, we are using bands ugrizy, and each band will have a name 'mag_u_lsst', for example, with the error column name being 'mag_err_u_lsst'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bba751",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "band_dict = {band: f\"mag_{band}_lsst\" for band in bands}\n",
    "rename_dict = {f\"mag_{band}_lsst_err\": f\"mag_err_{band}_lsst\" for band in bands}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6bedc",
   "metadata": {},
   "source": [
    "Now we can grab the galaxy data we'll use to train our creation model, and we'll rename the band columns to match our desired band names as discussed above, using `band_dict`. We can check that our columns have been renamed appropriately by printing out the first few lines of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87868116",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = get_galaxy_data().rename(band_dict, axis=1)\n",
    "# let's take a look at the columns\n",
    "catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1465f",
   "metadata": {},
   "source": [
    "Looks like the column names are the way we want them! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36776a",
   "metadata": {},
   "source": [
    "### Train and sample the model\n",
    "\n",
    "Now we need to use the galaxy data we retrieved to train the model that we'll use to create our input galaxy magnitude data catalogues later. We're going to use the `PZflow` engine to do this, specifically the `modeler` function. This will train the normalizing flow that serves as the engine for the input data creation. To get a sense of what it does and the parameters it needs, let's check out its docstrings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffff1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ri.creation.engines.flowEngine.flow_modeler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b35937",
   "metadata": {},
   "source": [
    "We'll pass the modeler a few parameters:\n",
    "- **input_data:** this is the input catalog that our modeler needs to train the data flow (the one we retrieved above)\n",
    "- **seed (optional):** this is the random seed used for training\n",
    "- **phys_cols (optional):** The names of any non-photometry columns and their [min,max] values.\n",
    "- **phot_cols (optional):** This is a dictionary of the names of the photometry columns and their corresponding [min,max] values.\n",
    "- **calc_colors (optional):** Whether to internally calculate colors (if phot_cols are magnitudes). Assumes that you want to calculate colors from adjacent columns in phot_cols. If you do not want to calculate colors, set False. Else, provide a dictionary `{‘ref_column_name’: band}`, where band is a string corresponding to the column in phot_cols you want to save as the overall galaxy magnitude. We're passing in the default value here just so you can see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_model = ri.creation.engines.flowEngine.flow_modeler(\n",
    "    input_data=catalog,\n",
    "    seed=0,\n",
    "    phys_cols={\"redshift\": [0, 3]},\n",
    "    phot_cols={\n",
    "        \"mag_u_lsst\": [17, 35],\n",
    "        \"mag_g_lsst\": [16, 32],\n",
    "        \"mag_r_lsst\": [15, 30],\n",
    "        \"mag_i_lsst\": [15, 30],\n",
    "        \"mag_z_lsst\": [14, 29],\n",
    "        \"mag_y_lsst\": [14, 28],\n",
    "    },\n",
    "    calc_colors={\"ref_column_name\": \"mag_i_lsst\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22be7cd",
   "metadata": {},
   "source": [
    "Now we'll use the flow to produce some synthetic data for our training data set and test data set. Since this is a test we'll create some small datasets, with 600 galaxies for this sample, so we'll pass in the argument: `n_samples = 600`. We'll also use a specific seed for each one to ensure they're reproducible but different from each other.\n",
    "\n",
    "**Note that when we pass the model to this function, we don't pass the dictionary, but the actual model object. This is true of all the interactive functions.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample test and training data sets\n",
    "train_data_orig = ri.creation.engines.flowEngine.flow_creator(\n",
    "    n_samples=600, model=flow_model[\"model\"], seed=1235\n",
    ")\n",
    "test_data_orig = ri.creation.engines.flowEngine.flow_creator(\n",
    "    model=flow_model[\"model\"], n_samples=600, seed=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d3582",
   "metadata": {},
   "source": [
    "Let's plot these data sets to check that they are in fact different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ecc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_options = {\"bins\": np.linspace(0, 3, 30), \"histtype\": \"stepfilled\", \"alpha\": 0.5}\n",
    "\n",
    "plt.hist(train_data_orig[\"output\"][\"redshift\"], label=\"train\", **hist_options)\n",
    "plt.hist(test_data_orig[\"output\"][\"redshift\"], label=\"test\", **hist_options)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"number of galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06077afc",
   "metadata": {},
   "source": [
    "## 2. Create \"degraded\" data sets by applying different sets of degraders\n",
    "\n",
    "Let's make 4 different training data sets using increasingly more degraders, and one set of test data with all four degradations applied. The degraders we'll be using are:\n",
    "\n",
    "1. `lsst_error_model` to add photometric errors that are modelled based on the telescope\n",
    "2. `inv_redshift_incompleteness` to mimic redshift dependent incompleteness\n",
    "3. `line_confusion` to simulate the effect of misidentified lines \n",
    "4. `quantity_cut` mimics a band-dependent brightness cut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604e17d",
   "metadata": {},
   "source": [
    "### 1. LSST Error Model\n",
    "\n",
    "This method adds photometric errors, non-detections and extended source errors that are modelled based on the Vera Rubin telescope. We're going to apply it to both training and test data sets. Once again, we're supplying different seeds to ensure the results are reproducible and different from each other. We need to supply the `band_dict` we created earlier, which tells the code what the band column names should be. We are also supplying `ndFlag=np.nan`, which just tells the code to make non-detections `np.nan` in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_data_photerrs = ri.creation.degraders.photometric_errors.lsst_error_model(\n",
    "    sample=train_data_orig[\"output\"], seed=66, renameDict=band_dict, ndFlag=np.nan\n",
    ")\n",
    "\n",
    "# test data set\n",
    "test_data_photerrs = ri.creation.degraders.photometric_errors.lsst_error_model(\n",
    "    sample=test_data_orig[\"output\"], seed=66, renameDict=band_dict, ndFlag=np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what the output looks like\n",
    "train_data_photerrs[\"output\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d320da",
   "metadata": {},
   "source": [
    "You can see that the error columns have been added in for each of the magnitude columns. \n",
    "\n",
    "Now let's take a look at what's happened to the magnitudes. Below we'll plot the u-band magnitudes before and after running the degrader. You can see that the higher magnitude objects now have a much wider variance in magnitude compared to their initial magnitudes, but at lower magnitudes they've remained similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to set the range because there are nans in the new dataset with errors, which messes up plt.hist2d\n",
    "range = [\n",
    "    [\n",
    "        np.min(train_data_orig[\"output\"][\"mag_u_lsst\"]),\n",
    "        np.max(train_data_orig[\"output\"][\"mag_u_lsst\"]),\n",
    "    ],\n",
    "    [\n",
    "        np.min(train_data_photerrs[\"output\"][\"mag_u_lsst\"]),\n",
    "        np.max(train_data_photerrs[\"output\"][\"mag_u_lsst\"]),\n",
    "    ],\n",
    "]\n",
    "plt.hist2d(\n",
    "    train_data_orig[\"output\"][\"mag_u_lsst\"],\n",
    "    train_data_photerrs[\"output\"][\"mag_u_lsst\"],\n",
    "    range=range,\n",
    "    bins=20,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.xlabel(\"original u-band magnitude\")\n",
    "plt.ylabel(\"new u-band magnitude\")\n",
    "plt.colorbar(label=\"number of galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cf36e",
   "metadata": {},
   "source": [
    "You can make this plot for all the other magnitudes if you'd like. \n",
    "\n",
    "### 2. Redshift Incompleteness \n",
    "\n",
    "This method applies a selection function, which keeps galaxies with probability $p_{\\text{keep}}(z) = \\min(1, \\frac{z_p}{z})$, where $z_p$ is the ''pivot'' redshift. We'll use $z_p = 1.0$. \n",
    "\n",
    "**NOTE**:\n",
    "This means that the output of the stage will have fewer galaxies than the input. We want to ensure that our degraded test data set matches our original \"true\" data set, so that the evaluation will be able to directly compare them. To do this, we can use the parameter `drop_rows=False`. This will return a data object of the same length as before, with a \"flag\" column that identifies which galaxies are to be kept, and which are to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7842538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data set\n",
    "train_data_inc = (\n",
    "    ri.creation.degraders.spectroscopic_degraders.inv_redshift_incompleteness(\n",
    "        sample=train_data_photerrs[\"output\"], pivot_redshift=1.0\n",
    "    )\n",
    ")\n",
    "\n",
    "# test data set - use drop_rows to ensure it's the same length\n",
    "test_data_inc = (\n",
    "    ri.creation.degraders.spectroscopic_degraders.inv_redshift_incompleteness(\n",
    "        sample=test_data_photerrs[\"output\"], pivot_redshift=1.0, drop_rows=False\n",
    "    )\n",
    ")\n",
    "test_data_inc[\"output\"]  # look at the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe515f",
   "metadata": {},
   "source": [
    "We can see that, as expected, the test data set has the \"flag\" column, and that the length of the data set is still 600. Now let's take a look at the training data set, where we left `drop_rows` as true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_inc[\"output\"]  # look at the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0ca88",
   "metadata": {},
   "source": [
    "This data set is shorter than the test data set now, since those galaxies have just been removed from the data entirely. This isn't a problem for the training data set, since we don't need to compare it to anything later. Let's plot a histogram of the training data set redshifts with just the photometric errors, and compare it to our new data set with both that and the redshift incompleteness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612653a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_photerrs[\"output\"][\"redshift\"], label=\"input\", **hist_options)\n",
    "plt.hist(train_data_inc[\"output\"][\"redshift\"], label=\"ouput\", **hist_options)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"number of galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2506cfd",
   "metadata": {},
   "source": [
    "The output data set clearly has fewer galaxies than the input data set above redshift of 1, and the distributions are the same for redshifts less than 1, as expected. \n",
    "\n",
    "For the test data set, we just have one more step that we need to do before we can feed it into any other degraders. We use the \"flag\" column to mask all of the \"dropped\" galaxy rows and set them all as `np.nan` - this keeps the indices the same, allowing us to compare to the truth data set as is our goal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the column as a separate variable\n",
    "inc_flag = test_data_inc[\"output\"][\"flag\"]\n",
    "\n",
    "# drop the flag column from the dataframe entirely\n",
    "test_data_inc[\"output\"].drop(columns=\"flag\", inplace=True)\n",
    "\n",
    "# replace the lines that are cut out by the degrader with np.nan\n",
    "new_test_data_inc = test_data_inc[\"output\"].where(inc_flag, np.nan)\n",
    "\n",
    "# take a look at the result\n",
    "new_test_data_inc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa7583",
   "metadata": {},
   "source": [
    "The new dataframe is the same length as the old one, but without the flag column, and now those rows will just be `np.nan`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21d1f7",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Line Confusion\n",
    "\n",
    "This method simulates the effect of misidentified lines. The degrader will misidentify some percentage (`frac_wrong`) of the actual lines (here we're picking `5007.0` Angstroms, which are OIII lines) as the line we pick for `wrong_wavelen`. In this case, we'll pick `3727.0` Angstroms, which are OII lines. \n",
    "\n",
    "This degrader doesn't cut any galaxies, so we don't have to worry about the `drop_rows` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 3: add in line confusion\n",
    "train_data_conf = ri.creation.degraders.spectroscopic_degraders.line_confusion(\n",
    "    sample=train_data_inc[\"output\"],\n",
    "    true_wavelen=5007.0,\n",
    "    wrong_wavelen=3727.0,\n",
    "    frac_wrong=0.05,\n",
    "    seed=1337,\n",
    ")\n",
    "\n",
    "# dataset 3: add in line confusion using the modified data set\n",
    "test_data_conf = ri.creation.degraders.spectroscopic_degraders.line_confusion(\n",
    "    sample=new_test_data_inc,\n",
    "    true_wavelen=5007.0,\n",
    "    wrong_wavelen=3727.0,\n",
    "    frac_wrong=0.05,\n",
    "    seed=1450,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ce6f50",
   "metadata": {},
   "source": [
    "Now let's take a look at what this has done to our redshift distribution by plotting the input training data set against the one output by the `line_confusion` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b75e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_inc[\"output\"][\"redshift\"], label=\"input data\", **hist_options)\n",
    "plt.hist(train_data_conf[\"output\"][\"redshift\"], label=\"output data\", **hist_options)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylabel(\"redshift\")\n",
    "plt.ylabel(\"number of galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6fa16",
   "metadata": {},
   "source": [
    "We can see that the output data has a few small differences in the distribution, spread across the whole range of redshifts. \n",
    "\n",
    "### 4. Quantity Cut\n",
    "\n",
    " This method cuts galaxies based on their band magnitudes. It takes a dictionary of cuts, where you can provide the band name and the values to cut that band on (for example, `{\"mag_i_lsst\": 25.0}`). If one value is given, it's considered a maximum, and if a tuple is given, it's considered a range within which the sample is selected. For this, we'll just set a maximum magnitude for the i band of 25.\n",
    "\n",
    " Since this method cuts galaxies, we're going to follow the steps we used for the `inv_redshift_incompleteness` method to keep our test dataset at the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut some of the data below a certain magnitude\n",
    "train_data_cut = ri.creation.degraders.quantityCut.quantity_cut(\n",
    "    sample=train_data_conf[\"output\"], cuts={\"mag_i_lsst\": 25.0}\n",
    ")\n",
    "\n",
    "# cut some of the data below a certain magnitude, set drop_rows=False to keep data set the same length\n",
    "test_data_cut = ri.creation.degraders.quantityCut.quantity_cut(\n",
    "    sample=test_data_conf[\"output\"], cuts={\"mag_i_lsst\": 25.0}, drop_rows=False\n",
    ")\n",
    "test_data_cut[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbfac45",
   "metadata": {},
   "source": [
    "We can see that there's been a flag column added to the test data again, but this time the flags are 1 and 0 instead of True and False. Let's save the flag column and drop it from the main DataFrame. We're going to do something a little different with the data later so we don't need do the `np.nan` substitution from earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159775f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save flag column\n",
    "cut_flag = test_data_cut[\"output\"][\"flag\"]\n",
    "\n",
    "# drop flag column from dataframe\n",
    "test_data_cut[\"output\"].drop(columns=\"flag\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27f787",
   "metadata": {},
   "source": [
    "Now let's plot a histogram of the training data set we input into the `quantity_cut` method compared to the output training data set to see how it's changed the number and distribution of galaxies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31f34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data_conf[\"output\"][\"redshift\"], label=\"input data\", **hist_options)\n",
    "plt.hist(train_data_cut[\"output\"][\"redshift\"], label=\"output data\", **hist_options)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"number of galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b9bc9",
   "metadata": {},
   "source": [
    "We can see our output distribution has roughly the same shape, but with significantly fewer galaxies overall. \n",
    "\n",
    "Now we have applied four different degraders, so we've set up our various training data sets, and our test data set. The final step is to use the dictionary we made earlier of error column names (`rename_dict`) to rename the error columns, so they match the expected names for the later steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5df60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renames error columns to match DC2 for training data sets\n",
    "\n",
    "# photerrs\n",
    "df_train_data_photerrs = ri.tools.table_tools.column_mapper(\n",
    "    data=train_data_photerrs[\"output\"], columns=rename_dict\n",
    ")\n",
    "\n",
    "# photerrs\n",
    "df_train_data_inc = ri.tools.table_tools.column_mapper(\n",
    "    data=train_data_inc[\"output\"], columns=rename_dict\n",
    ")\n",
    "\n",
    "# photerrs\n",
    "df_train_data_conf = ri.tools.table_tools.column_mapper(\n",
    "    data=train_data_conf[\"output\"], columns=rename_dict\n",
    ")\n",
    "\n",
    "# photerrs\n",
    "df_train_data_cut = ri.tools.table_tools.column_mapper(\n",
    "    data=train_data_cut[\"output\"], columns=rename_dict\n",
    ")\n",
    "\n",
    "\n",
    "# renames error columns for test data set\n",
    "df_test_data = ri.tools.table_tools.column_mapper(\n",
    "    data=test_data_cut[\"output\"], columns=rename_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749b528",
   "metadata": {},
   "source": [
    "Now that we have all four of our training data sets, let's plot them all together to get a final look at their differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f619513",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    df_train_data_photerrs[\"output\"][\"redshift\"],\n",
    "    label=\"photometric errors\",\n",
    "    **hist_options,\n",
    ")\n",
    "plt.hist(\n",
    "    df_train_data_inc[\"output\"][\"redshift\"], label=\"z incompleteness\", **hist_options\n",
    ")\n",
    "plt.hist(\n",
    "    df_train_data_conf[\"output\"][\"redshift\"], label=\"line confusion\", **hist_options\n",
    ")\n",
    "plt.hist(df_train_data_cut[\"output\"][\"redshift\"], label=\"quantity cut\", **hist_options)\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"number of galaxies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce5fa1",
   "metadata": {},
   "source": [
    "We have one final step to do to our test data sets before we can use them in the estimation and evaluation stages. For these data sets to work with the RAIL estimate and evaluate stages, we want a couple of things:\n",
    "1. Our degraded (and cut down) test DataFrame indices to match up with our original test DataFrame indices\n",
    "2. Our test data sets to not have columns with all NaNs \n",
    "3. Our test data to have linearly increasing indices (i.e. not retain the masked indices) \n",
    "\n",
    "In order to accomplish this, we're going to do the following:\n",
    "1. Mask the degraded test data set using our existing masks \n",
    "2. Mask the \"truth\" test data set using the existing masks \n",
    "3. Reindex both of these arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f939e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the degraded test data\n",
    "masked_test_data = df_test_data[\"output\"][cut_flag & inc_flag]\n",
    "\n",
    "# reset the index\n",
    "reindexed_test_data = masked_test_data.reset_index(drop=True)\n",
    "reindexed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the degraded test data\n",
    "masked_test_data_orig = test_data_orig[\"output\"][cut_flag & inc_flag]\n",
    "\n",
    "# reset the index\n",
    "reindexed_test_data_orig = masked_test_data_orig.reset_index(drop=True)\n",
    "reindexed_test_data_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba881fb",
   "metadata": {},
   "source": [
    "We can see that these DataFrames are now the same length, with indices that actually match the length of the arrays and that are linearly increasing, which is what we wanted. Now these can be appropriately compared to each other in the later steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25f641",
   "metadata": {},
   "source": [
    "## 3. Use the data sets to estimate redshifts\n",
    "\n",
    "Now we can loop through the two steps required to estimate redshifts: **informing** the model and using the model to **estimate**. We'll use all four of our training data sets to train our models. For this notebook, we'll use the [K-Nearest Neighbours](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html#k-nearest-neighbor) (KNN) algorithm, which is a wrapper around `sklearn`'s nearest neighbour (NN) machine learning model. Essentially, it takes a given galaxy, identifies its nearest neighbours in the space, in this case galaxies that have similar colours, and then constructs the photometric redshift PDF as a sum of Gaussians from each neighbour. For more details on how this algorithm works, you can see the [wikipedia page](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) or the [Quick Start in Estimation](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/estimation_examples/00_Quick_Start_in_Estimation.html) notebook.\n",
    "\n",
    "**Inform**: The inform method is training the model that we will use to estimate the redshifts. We will plug in our training data sets, and any parameters the model needs.\n",
    "\n",
    "**Estimate**: Once our model is trained, we can then use it to estimate the redshifts of the test data set. We provide the estimate algorithm with the test data set, and the model that we've trained, and any other necessary parameters.\n",
    "\n",
    "Common parameters:\n",
    "- `nondetect_val`: This tells the code which values are considered non-detections. We pass in `np.nan` here, since that's what we used as the `ndFlag` in the degradation stage for non-detections. \n",
    "- `hdf5_groupname`: the dictionary key the code will find the data under. Set to `\"\"` if the data is passed in directly. \n",
    "\n",
    "First, we'll set up a dictionary with all four of the training datasets, and empty dictionaries to store the estimated redshift distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of the training datasets to iterate through\n",
    "train_datasets = {\n",
    "    \"lsst_error_model\": df_train_data_photerrs,\n",
    "    \"inv_redshift_inc\": df_train_data_inc,\n",
    "    \"line_confusion\": df_train_data_conf,\n",
    "    \"quantity_cut\": df_train_data_cut,\n",
    "}\n",
    "\n",
    "# set up dictionary for output\n",
    "estimated_photoz = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982d92f",
   "metadata": {},
   "source": [
    "Now we'll iterate through the datasets, calibrating the model with each training set, and then using that model to generate estimated redshifts for our test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040eca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in train_datasets.items():\n",
    "\n",
    "    # train the model\n",
    "    inform_knn = ri.estimation.algos.k_nearneigh.k_near_neigh_informer(\n",
    "        training_data=df[\"output\"], nondetect_val=np.nan, hdf5_groupname=\"\"\n",
    "    )\n",
    "    # estimate the photozs\n",
    "    knn_estimated = ri.estimation.algos.k_nearneigh.k_near_neigh_estimator(\n",
    "        input_data=reindexed_test_data,\n",
    "        model=inform_knn[\"model\"],\n",
    "        nondetect_val=np.nan,\n",
    "        hdf5_groupname=\"\",\n",
    "    )\n",
    "    estimated_photoz[key] = knn_estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219e4217",
   "metadata": {},
   "source": [
    "Now let's take a look at what the output of the estimation stage actually looks like. Most estimation stages output an `Ensemble`, which is a data structure from the package `qp`. For more information, see [the qp documentation](https://qp.readthedocs.io/en/main/user_guide/datastructure.html). \n",
    "\n",
    "We're using an `Ensemble` to hold a redshift distribution for each of the galaxies we're estimating. There are two required dictionaries that make up an Ensemble, and one that is optional:\n",
    "- `.metadata`: Contains information about the whole data structure, like the Ensemble type, and any shared parameters such as the bins of histograms. This is not per-object metadata. \n",
    "- `.objdata`: The main data points of the distributions for each object, where each object is a row. \n",
    "- `.ancil`: the optional dictionary, containing extra information about each object. It can have arrays that have one or more data points per distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated_photoz contains the output of the KNN estimate function for each of our\n",
    "# parameter sets. Here we print out the result for just one of them. We can see that\n",
    "# the Ensemble has the same number of rows as galaxies that we input, and some number\n",
    "# of points per row\n",
    "print(estimated_photoz[\"lsst_error_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa555c9",
   "metadata": {},
   "source": [
    "We can see that this algorithm outputs Ensembles of class `mixmod`, which are just combinations of Gaussians (for more info see the [qp docs](https://qp.readthedocs.io/en/main/user_guide/parameterizations/mixmod.html)). \n",
    "\n",
    "So each distribution in this Ensemble has a set of Gaussians that, added together, make up the distribution. Each distribution is therefore described by a set of means, weights, and standard deviations. The shape portion of the print statement tells us two things: the first number is the number of photo-z distributions, or galaxies, in this `Ensemble`, and the second number tells us how many Gaussians are combined to make up each photo-z distribution. \n",
    "\n",
    "Let's take a look at what the different dictionaries look like for this `Ensemble`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b98b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the metadata dictionary of that output Ensemble\n",
    "print(estimated_photoz[\"lsst_error_model\"][\"output\"].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cdcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the actual distribution data of that output Ensemble, which contains\n",
    "# the data points that describe each photometric redshift probability distribution\n",
    "print(estimated_photoz[\"lsst_error_model\"][\"output\"].objdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a7abe",
   "metadata": {},
   "source": [
    "Typically the ancillary data table includes a photo-z point estimate derived from the PDFs, by default this is the mode of the distribution, called 'zmode' in the ancillary dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98781f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the ancillary dictionary of the output Ensemble, which in this case\n",
    "# contains the zmode, redshift, and distribution type\n",
    "print(estimated_photoz[\"lsst_error_model\"][\"output\"].ancil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a766d",
   "metadata": {},
   "source": [
    "Now let's plot one redshift PDF from each of our four estimated redshift distribution datasets to compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.linspace(0, 3, 200)  # we want to cover the whole available redshift space\n",
    "for key, df in estimated_photoz.items():\n",
    "    plt.plot(xvals, df[\"output\"][100].pdf(xvals), label=key)\n",
    "\n",
    "# plot the true redshift\n",
    "plt.axvline(\n",
    "    test_data_orig[\"output\"][\"redshift\"].iloc[0],\n",
    "    color=\"k\",\n",
    "    ls=\"--\",\n",
    "    label=\"true redshift\",\n",
    ")\n",
    "\n",
    "plt.legend(loc=\"best\", title=\"training dataset\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"p(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad98eae",
   "metadata": {},
   "source": [
    "This plot shows us the estimated photo-z PDF for the first galaxy with each of the different training sets, compared to the redshift from the \"true\" test dataset we sampled at the beginning. \n",
    "\n",
    "Plotting one distribution at a time isn't the best way to get a sense of how the whole set of galaxy redshift distributions changes, so let's summarize these distributions. This will give us a sense of how all of the estimated redshift distributions change with each different training data set. There are a number of summarizing algorithms, but here we'll use two of the most basic: \n",
    "\n",
    "1. [**Point Estimate Histogram**](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html#point-estimate-histogram): This algorithm creates a histogram of all the point estimates of the photometric redshifts. By default, the point estimate used is `zmode`, which is usually found in the ancillary dictionary of the distributions. \n",
    "2. [**Naive Stacking**](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/estimation.html#naive-stacking): This algorithm stacks the PDFs of the estimated photometric redshifts together and normalizes the stacked distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dictionaries for output\n",
    "point_est_dict = {}\n",
    "naive_stack_dict = {}\n",
    "\n",
    "for key, df in train_datasets.items():\n",
    "\n",
    "    # get the summary of the point estimates\n",
    "    point_estimate_ens = ri.estimation.algos.point_est_hist.point_est_hist_summarizer(\n",
    "        input_data=estimated_photoz[key][\"output\"]\n",
    "    )\n",
    "    point_est_dict[key] = point_estimate_ens\n",
    "\n",
    "    # get a summary of the PDFs\n",
    "    naive_stack_ens = ri.estimation.algos.naive_stack.naive_stack_summarizer(\n",
    "        input_data=estimated_photoz[key][\"output\"]\n",
    "    )\n",
    "    naive_stack_dict[key] = naive_stack_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de59ddc",
   "metadata": {},
   "source": [
    "Now let's take a look at the output dictionaries for both these functions for one of the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(point_est_dict[\"lsst_error_model\"])\n",
    "print(naive_stack_dict[\"lsst_error_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691b7a94",
   "metadata": {},
   "source": [
    "These functions output `Ensembles`, just like the KNN estimation algorithm. However, they output two separate `Ensembles`: the 'single_NZ' one contains just one distribution, the actual stacked distribution that has been created. The 'output' one contains a number of bootstrapped distributions, to make further analysis easier.\n",
    "\n",
    "We're going to focus on the 'single_NZ' distribution here. We'll start by plotting the point estimate summarized distributions for all of the runs, which are histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bin centers and widths\n",
    "bin_width = (\n",
    "    point_est_dict[\"lsst_error_model\"][\"single_NZ\"].metadata[\"bins\"][1]\n",
    "    - point_est_dict[\"lsst_error_model\"][\"single_NZ\"].metadata[\"bins\"][0]\n",
    ")\n",
    "bin_centers = (\n",
    "    point_est_dict[\"lsst_error_model\"][\"single_NZ\"].metadata[\"bins\"][:-1]\n",
    "    + point_est_dict[\"lsst_error_model\"][\"single_NZ\"].metadata[\"bins\"][1:]\n",
    ") / 2\n",
    "\n",
    "for key, df in point_est_dict.items():\n",
    "    plt.bar(\n",
    "        bin_centers,\n",
    "        df[\"single_NZ\"].objdata[\"pdfs\"],\n",
    "        width=bin_width,\n",
    "        alpha=0.7,\n",
    "        label=key,\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"N(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f5d6d",
   "metadata": {},
   "source": [
    "It's a little difficult to see the differences between so many distributions in this format, but you can get a sense that there are some distinct differences in the distributions of redshifts. \n",
    "\n",
    "Let's plot the summarized distributions from the Naive Stacking algorithm, which are smoothed distributions since they are created by stacking the full photo-z PDFs instead of point estimates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be95723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in naive_stack_dict.items():\n",
    "    plt.plot(\n",
    "        df[\"single_NZ\"].metadata[\"xvals\"], df[\"single_NZ\"].objdata[\"yvals\"], label=key\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"N(z)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad87a107",
   "metadata": {},
   "source": [
    "It's a bit easier to see the differences between the distributions of redshifts in this plot. We can see that the overall shape of the distributions is the same, but there are some significant differences, in particular at higher redshifts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a20e6f7",
   "metadata": {},
   "source": [
    "## 4. Evaulate the estimated redshift distributions and compare them \n",
    " **ADD LINKS TO EVALUATION NOTEBOOKS** \n",
    "Now that we have a sense of how the distributions of the estimated photometric redshift probability distributions differ, let's get a little more technical. We'll use the `Evaluator` stage to calculate some metrics for each of the distributions of redshifts. For a more detailed look at the available metrics and how to use them, take a look at the 01_Evaluation_by_Type.ipynb notebook. \n",
    "\n",
    "Here are the metrics we'll calculate:\n",
    "1. The [Brier score](https://en.wikipedia.org/wiki/Brier_score), which assesses the accuracy of probabilistic predictions. The lower the score, the better the predictions.  \n",
    "2. The [Conditional Density Estimation loss](https://vitaliset.github.io/conditional-density-estimation/), which is the averaged squared loss between the true and predicted conditional probability density functions. The lower the score, the better the predicted probability density, in this case, the photometric redshift distributions.\n",
    "\n",
    "For the evaluation metrics, in general we need the estimated redshift distributions, and the actual redshifts -- these are the pre-degradation redshifts from our initially sampled distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44600a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dictionaries for output\n",
    "eval_dict = {}\n",
    "\n",
    "for key, df in train_datasets.items():\n",
    "    # evaluate the results\n",
    "    evaluator_stage_dict = dict(\n",
    "        metrics=[\"cdeloss\", \"brier\"],\n",
    "        _random_state=None,\n",
    "        metric_config={\n",
    "            \"brier\": {\"limits\": (0, 3.1)},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    the_eval = ri.evaluation.dist_to_point_evaluator.dist_to_point_evaluator(\n",
    "        data=estimated_photoz[key][\"output\"],\n",
    "        truth=reindexed_test_data_orig,\n",
    "        **evaluator_stage_dict,\n",
    "        hdf5_groupname=\"\",\n",
    "    )\n",
    "\n",
    "    # put the evaluation results in a dictionary so we have them\n",
    "    eval_dict[key] = the_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35d0c5",
   "metadata": {},
   "source": [
    "Now let's take a look at the metrics we calculated, and compare them. The code below just selects the one dictionary output per run that we want to look at, to make the dictionary a little easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388814cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data out of the sub-directory to make the dictionaries easier to read\n",
    "results_dict = {key: val[\"summary\"] for key, val in eval_dict.items()}\n",
    "\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ab0fa",
   "metadata": {},
   "source": [
    "We can also plot these metrics to better visualize which of the runs has better scores, 'better' here meaning lower for both of the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93716d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in eval_dict.items():\n",
    "    plt.scatter(value[\"summary\"][\"brier\"], value[\"summary\"][\"cdeloss\"], label=key)\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Brier score\")\n",
    "plt.ylabel(\"CDE loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1225af",
   "metadata": {},
   "source": [
    "This gives us a bit of a clearer picture of which training distributions did better than others. It's also clear from this why multiple metrics can be useful, since some of these distributions do better in one metric than the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27674406",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If you'd like to parallelize your iteration in order to speed things up, take a look at the [introduction to RAIL interactive](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/estimation_examples/Estimating_Redshifts_and_Comparing_Results_for_Different_Parameters.html) notebook. \n",
    "\n",
    "To learn more about the creation stage of RAIL, and the available degraders, take a look at the [RAIL Creation docs](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/creation.html). \n",
    "\n",
    "Similarly, if you'd like to learn more about the Evaluation stage, you can take a look at the [RAIL Evaluation docs](https://rail-hub.readthedocs.io/en/latest/source/rail_stages/evaluation.html), or try out the [Evaluation by type](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/interactive_examples/rendered/evaluation_examples/01_Evaluation_by_Type.html) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
